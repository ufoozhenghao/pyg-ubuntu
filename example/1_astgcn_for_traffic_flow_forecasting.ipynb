{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n",
    "import sys\n",
    "print(\"Python version\")\n",
    "print(sys.version)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "import os\n",
    "!pip install torch_geometric\n",
    "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
    "\n",
    "# 更改当前工作目录\n",
    "# os.chdir(\"/content/drive/MyDrive/ColabNotebooks/GNN/pyg_install-whl\")\n",
    "# !pip install pyg_lib-0.4.0+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "# !pip install torch_spline_conv-1.2.2+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "# !pip install torch_scatter-2.1.2+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "# !pip install torch_sparse-0.6.18+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "# !pip install torch_cluster-1.6.3+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "!pip install torch-geometric-temporal\n",
    "# !pip install -q git+https://github.com/elmahyai/pytorch_geometric_temporal"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1>\n",
    "<center>Attention Based Spatial-Temporal Graph Convolutional Networks\n",
    "for Traffic Flow Forecasting</center>\n",
    "</h1>\n",
    "\n",
    "In this notebook we will dive into attentional temporal graph convolution networks where everything new meets Attention + deep learning time series analysis( temporal data) + Graph convolution all in one thing. This is a rewriting of the code of the paper (Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting https://ojs.aaai.org/index.php/AAAI/article/view/3881 ), from which we will include quotes and parts of their code. This notebook uses the data prepared in the previous notebook (Processing traffic data for deep learning projects)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "!pip install tensorboardX"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0')\n",
    "print(\"CUDA:\", USE_CUDA, DEVICE)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "sw = SummaryWriter(logdir='.', flush_secs=5)\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.typing import OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.transforms import LaplacianLambdaMax\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, get_laplacian\n",
    "from torch_geometric.utils import to_dense_adj"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading data  ( The temporal part)\n",
    "As discussed in the previous notebook, the data is splitted as follows:\n",
    "\n",
    "10181 data/target examples will be used as the training set ( 35 days )\n",
    "\n",
    "3394 data/target examples will be used as the validation set (12 days)\n",
    "\n",
    "3394 data/target examples will be used as the testing set (12 days)\n",
    "\n",
    "The shape for each prediction / target hour example is  (12, 307, 3)  , As the data will be loaded in batches of size 32\n",
    "\n",
    "its shape will be (32, 12, 307, 3)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "加载图信号数据，并将其转换为 PyTorch 的 DataLoader\n",
    "流程：\n",
    "1.转换数据类型：将 NumPy 数组转换为 PyTorch 张量，并确保它们的数据类型为浮点数。\n",
    "2.移动到设备：将张量移动到指定的设备（如 GPU）。\n",
    "3.创建数据集：使用 TensorDataset 将输入张量和目标张量配对，创建一个数据集对象。\n",
    "4.创建数据加载器：使用 DataLoader 将数据集分成小批量，并设置是否打乱数据。\n",
    "'''\n",
    "def load_graphdata_channel1(graph_signal_matrix_filename, num_of_hours, num_of_days, num_of_weeks, batch_size,\n",
    "                            shuffle=True, DEVICE = torch.device('cuda:0')):\n",
    "    '''\n",
    "    :param graph_signal_matrix_filename: str\n",
    "    :param num_of_hours: int\n",
    "    :param num_of_days: int\n",
    "    :param num_of_weeks: int\n",
    "    :param DEVICE:\n",
    "    :param batch_size: int\n",
    "    shuffle：是否在训练数据加载器中打乱数据。\n",
    "\n",
    "    :return:\n",
    "    three DataLoaders, each dataloader contains:\n",
    "    test_x_tensor: (B, N_nodes, in_feature, T_input)\n",
    "    test_decoder_input_tensor: (B, N_nodes, T_output)\n",
    "    test_target_tensor: (B, N_nodes, T_output)\n",
    "    '''\n",
    "\n",
    "    file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "    current_directory = os.getcwd()\n",
    "    print(\"当前工作目录:\", current_directory)\n",
    "    filename = os.path.join('./data/pems-dataset/PEMS04/', file + '_r' + str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(num_of_weeks)) +'_astcgn'\n",
    "    print('load file:', filename)\n",
    "    file_data = np.load(filename + '.npz')\n",
    "\n",
    "    train_x = file_data['train_x']  # (10181, 307, 3, 12)\n",
    "    train_x = train_x[:, :, 0:1, :]\n",
    "    train_target = file_data['train_target']  # (10181, 307, 12)\n",
    "\n",
    "    val_x = file_data['val_x']\n",
    "    val_x = val_x[:, :, 0:1, :]\n",
    "    val_target = file_data['val_target']\n",
    "\n",
    "    test_x = file_data['test_x']\n",
    "    test_x = test_x[:, :, 0:1, :]\n",
    "    test_target = file_data['test_target']\n",
    "\n",
    "    mean = file_data['mean'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "    std = file_data['std'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "\n",
    "    # ------- train_loader -------\n",
    "    train_x_tensor = torch.from_numpy(train_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    train_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    '''\n",
    "    torch.from_numpy 将 NumPy 数组转换为 PyTorch 张量。\n",
    "    .type(torch.FloatTensor) 将张量的数据类型转换为 FloatTensor，即 32 位浮点数\n",
    "    .to(DEVICE) 将张量移动到指定的设备（如 GPU 或 CPU）\n",
    "    torch.utils.data.TensorDataset 创建一个数据集对象，该对象将输入张量和目标张量配对,返回一个TensorDataset对象\n",
    "    TensorDataset 是PyTorch中的数据集类，用于将多个张量组合成一个数据集。每个样本由输入张量和目标张量组成。\n",
    "    torch.utils.data.DataLoader 创建一个数据加载器，用于批量加载数据\n",
    "if loss_function=='masked_mse':    DataLoader 是PyTorch中的一个类，用于将数据集分成小批量，并在训练过程中方便地迭代数据。\n",
    "    '''\n",
    "\n",
    "\n",
    "    # ------- val_loader -------\n",
    "    val_x_tensor = torch.from_numpy(val_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    val_target_tensor = torch.from_numpy(val_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_x_tensor, val_target_tensor)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ------- test_loader -------\n",
    "    test_x_tensor = torch.from_numpy(test_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    test_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # print 打印数据尺寸\n",
    "    print('train:', train_x_tensor.size(), train_target_tensor.size())\n",
    "    print('val:', val_x_tensor.size(), val_target_tensor.size())\n",
    "    print('test:', test_x_tensor.size(), test_target_tensor.size())\n",
    "\n",
    "    return train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, mean, std\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# 更改当前工作目录\n",
    "os.chdir(\"/content/drive/MyDrive/ColabNotebooks/pytorch_geometric_temporal/notebooks\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "graph_signal_matrix_filename = './data/pems-dataset/PEMS04/PEMS04.npz'\n",
    "batch_size = 32\n",
    "num_of_weeks = 0\n",
    "num_of_days = 0\n",
    "num_of_hours = 1\n",
    "\n",
    "train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, _mean, _std = load_graphdata_channel1(\n",
    "    graph_signal_matrix_filename, num_of_hours, num_of_days, num_of_weeks, batch_size)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading the graph ( The spatial part)\n",
    "\n",
    "In our case the graph is the Traffic Networks (literally, the network between the detectors (sensors) applied on the traffic networks\n",
    "\n",
    "In our example we have 307 detectors that when connected spatially, give our traffic network under investigation.\n",
    "\n",
    "# 加载图表（空间部分）\n",
    "\n",
    "在我们的例子中，该图是交通网络（字面意思是交通网络上应用的检测器（传感器）之间的网络\n",
    "\n",
    "在我们的示例中，我们有 307 个探测器，当它们在空间上连接时，可以对我们的交通网络进行调查。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "加载图表（空间部分）\n",
    "\n",
    "在我们的例子中，该图是交通网络（字面意思是交通网络上应用的检测器（传感器）之间的网络\n",
    "\n",
    "在我们的示例中，我们有 307 个探测器，当它们在空间上连接时，可以对我们的交通网络进行调查。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_adjacency_matrix(distance_df_filename, num_of_vertices, id_filename=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_df_filename: str, path of the csv file contains edges information\n",
    "    num_of_vertices: int, the number of vertices\n",
    "    Returns\n",
    "    ----------\n",
    "    A: np.ndarray, adjacency matrix\n",
    "    '''\n",
    "    if 'npy' in distance_df_filename:  # false\n",
    "        adj_mx = np.load(distance_df_filename)\n",
    "        return adj_mx, None\n",
    "    else:\n",
    "\n",
    "        #--------------------------------------------- read from here\n",
    "        import csv\n",
    "        A = np.zeros((int(num_of_vertices), int(num_of_vertices)),dtype=np.float32)\n",
    "        distaneA = np.zeros((int(num_of_vertices), int(num_of_vertices)), dtype=np.float32)\n",
    "\n",
    "        #------------ Ignore\n",
    "        if id_filename: # false\n",
    "            with open(id_filename, 'r') as f:\n",
    "                id_dict = {int(i): idx for idx, i in enumerate(f.read().strip().split('\\n'))}  # 把节点id（idx）映射成从0开始的索引\n",
    "\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[id_dict[i], id_dict[j]] = 1\n",
    "                    distaneA[id_dict[i], id_dict[j]] = distance\n",
    "            return A, distaneA\n",
    "\n",
    "        else:\n",
    "         #-------------Continue reading\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[i, j] = 1\n",
    "                    distaneA[i, j] = distance\n",
    "            return A, distaneA"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "id_filename = None\n",
    "# 邻接矩阵文件的路径，即存储交通网络连接信息的文件。\n",
    "adj_filename = './data/pems-dataset/PEMS04/PEMS04.csv'\n",
    "# num_of_vertices：节点的数量，即传感器的数量\n",
    "num_of_vertices = 307\n",
    "# get_adjacency_matrix：用于读取邻接矩阵文件并生成邻接矩阵adj_mx和距离矩阵distance_mx。邻接矩阵adj_mx是一个307x307的矩阵，表示节点之间的连接关系。\n",
    "adj_mx, distance_mx = get_adjacency_matrix(adj_filename, num_of_vertices, id_filename) #  adj_mx and distance_mx (307, 307)\n",
    "\n",
    "# 使用 networkx 库创建图并绘制出来。\n",
    "# 找到邻接矩阵中值为1的位置，这些位置表示节点之间的连接。\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "# 将行和列索引组合成边的列表。\n",
    "edges = zip(rows.tolist(), cols.tolist())\n",
    "gr = nx.Graph()\n",
    "gr.add_edges_from(edges)\n",
    "nx.draw(gr, node_size=3)\n",
    "plt.show()\n",
    "\n",
    "# 创建PyTorch张量表示边的索引\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "edges = zip(rows.tolist(), cols.tolist()) # 将行和列索引组合成边的列表。\n",
    "# torch.LongTensor(np.array([rows, cols]))：将边的行和列索引转换为PyTorch的长整型张量。\n",
    "edge_index_data = torch.LongTensor(np.array([rows, cols])).to(DEVICE)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Making the model"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Layers"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Temporal attention layer\n",
    "\n",
    "In the temporal dimension, there exist correlations between the traffic conditions in different time slices, and the correlations are also varying under different situations. Likewise, we use an attention mechanism to adaptively attach different importance to data.\n",
    "\n",
    "在时间维度上，不同时间片内的交通状况之间存在相关性，且不同情况下相关性也不同。同样，我们使用注意力机制来自适应地对数据赋予不同的重要性\n",
    "\n",
    "<img src=\"https://i.ibb.co/KwXCqJx/temp-attention.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To understand the equation :\n",
    "\n",
    "It learns to attend (focus) on which part of the time segement used as input. In our case we have 12 time points So it will generate 12 by 12 weights.\n",
    "\n",
    "要理解这个方程：\n",
    "\n",
    "它学习关注（关注）时间段的哪一部分用作输入。在我们的例子中，我们有 12 个时间点，因此它将生成 12 x 12 的权重。\n",
    "\n",
    "<img src=\"https://i.ibb.co/NZ4fh4k/atten2.jpg\" width=\"400\">"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Spatial attention layer\n",
    "\n",
    "In the spatial dimension, the traffic conditions of different locations have influence among each other and the mutual influence is highly dynamic. Here, we use an attention mechanism (Feng et al. 2017) to adaptively capture the dynamic correlations between nodes in the spatial dimension.\n",
    "\n",
    "空间注意力层\n",
    "\n",
    "在空间维度上，不同地点的交通状况相互影响，且相互影响是高度动态的。在这里，我们使用注意力机制（Feng et al. 2017）来自适应捕获空间维度中节点之间的动态相关性。\n",
    "\n",
    "<img src=\"https://i.ibb.co/PGnj4MR/spatial1.png\" width=\"400\">\n",
    "\n",
    "<img src=\"https://i.ibb.co/G5jkKvr/spatial2.png\" width=\"400\">\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The same as with the temporal attention; however, here the attention weights will be used inside a Graph convolution layer\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/stTfTFM/spat2.jpg\" width=\"400\">\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Spectral graph analysis on the spatial part\n",
    "Since the spatial part is represented as a graph, we will apply graph convolution to aggregate messages from neighbor nodes. The type of graph convolution that we are going to use is spectral convolution.\n",
    "\n",
    "* In spectral graph analysis, a graph is represented by its corresponding Laplacian matrix.\n",
    "* The properties of the graph structure can be obtained by analyzing Laplacian matrix and its eigenvalues\n",
    "\n",
    "* Laplacian matrix of a graph is defined as L = D − A,\n",
    "\n",
    "* Its normalized form is L = I − ((1/ sqrt(D) A ( 1/ sqrt(D))\n",
    "\n",
    "where A is the adjacent matrix, I is a unit matrix, and the degree matrix D (diagnoal diagonal matrix, consisting of node degrees,at the diagonal)\n",
    "\n",
    "The eigenvalue decomposition of the Laplacian matrix is L = U*Λ*(U.transpose()) , where Λ = diag([λ0, ..., λN −1]) is a diagonal matrix, and U is Fourier basis.\n",
    "\n",
    "U is an orthogonal matrix.\n",
    "\n",
    "The graph convolution is a convolution operation implemented by using linear operators that diagonalize in the Fourier domain to replace the classical convolution operator.\n",
    "\n",
    "However, it is expensive to directly perform the eigenvalue decomposition on the Laplacian matrix when the scale of the graph is large. Therefore, Chebyshev polynomials are adopted to solve this problem approximately but efficiently.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The ASTGCN model structure\n",
    "\n",
    "\n",
    "The model is composed of two ASTGCN blocks followed by a final layer\n",
    "\n",
    "Original x (input) is (32, 307, 1, 12) -Block1> (32, 307, 64, 12) -Block2> (32, 307, 64, 12) -permute-> (32, 12, 307,64)\n",
    "            # -final_conv-> (32, 12, 307, 1) -reshape-> (32,307,12) \"The target\"\n",
    "\n",
    "该模型是具有相同结构的三个独立组件的融合，旨在分别对历史数据的近期、日周期和周周期依赖性进行建模。这在之前的笔记本中已经讨论过\n",
    "\n",
    "The model is  the fusion of three independent components with the same structure, which are designed to respectively model the recent, daily-periodic and weekly-periodic dependencies of the historical data. This is discussed in the previous notebook (https://www.kaggle.com/elmahy/processing-traffic-data-for-deep-learning-projects).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xOXbOtZPPxN1",
    "nOzGOD_YPxN2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "papermill": {
   "duration": 1234.454996,
   "end_time": "2022-01-02T19:59:18.744009",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-02T19:38:44.289013",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
