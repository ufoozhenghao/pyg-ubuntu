{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n",
    "import sys\n",
    "print(\"Python version\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "!pip install torch_geometric\n",
    "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
    "\n",
    "# 更改当前工作目录\n",
    "# os.chdir(\"/content/drive/MyDrive/ColabNotebooks/GNN/pyg_install-whl\")\n",
    "# !pip install pyg_lib-0.4.0+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "# !pip install torch_spline_conv-1.2.2+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "# !pip install torch_scatter-2.1.2+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "# !pip install torch_sparse-0.6.18+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "# !pip install torch_cluster-1.6.3+pt24cu121-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "!pip install torch-geometric-temporal\n",
    "# !pip install -q git+https://github.com/elmahyai/pytorch_geometric_temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ci2W1QDBPxNy",
    "papermill": {
     "duration": 0.020249,
     "end_time": "2022-01-02T19:39:28.951508",
     "exception": false,
     "start_time": "2022-01-02T19:39:28.931259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1>\n",
    "<center>Attention Based Spatial-Temporal Graph Convolutional Networks\n",
    "for Traffic Flow Forecasting</center>\n",
    "</h1>\n",
    "\n",
    "In this notebook we will dive into attentional temporal graph convolution networks where everything new meets Attention + deep learning time series analysis( temporal data) + Graph convolution all in one thing. This is a rewriting of the code of the paper (Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting https://ojs.aaai.org/index.php/AAAI/article/view/3881 ), from which we will include quotes and parts of their code. This notebook uses the data prepared in the previous notebook (Processing traffic data for deep learning projects)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:50:02.247616Z",
     "start_time": "2024-11-01T06:50:02.240128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4721,
     "status": "ok",
     "timestamp": 1727250008708,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "kOFIxvlLPxNz",
    "outputId": "9eb201f3-ff52-4d4a-d843-ce96ce415c33",
    "papermill": {
     "duration": 7.749484,
     "end_time": "2022-01-02T19:39:36.721075",
     "exception": false,
     "start_time": "2022-01-02T19:39:28.971591",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-01T06:43:09.182277Z",
     "start_time": "2024-11-01T06:43:09.103412Z"
    }
   },
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0')\n",
    "print(\"CUDA:\", USE_CUDA, DEVICE)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "sw = SummaryWriter(logdir='.', flush_secs=5)\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.typing import OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.transforms import LaplacianLambdaMax\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, get_laplacian\n",
    "from torch_geometric.utils import to_dense_adj"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m time\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnetworkx\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnx\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'numpy'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y37jtiwpPxNz",
    "papermill": {
     "duration": 0.022775,
     "end_time": "2022-01-02T19:39:36.766042",
     "exception": false,
     "start_time": "2022-01-02T19:39:36.743267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading data  ( The temporal part)\n",
    "As discussed in the previous notebook, the data is splitted as follows:\n",
    "\n",
    "10181 data/target examples will be used as the training set ( 35 days )\n",
    "\n",
    "3394 data/target examples will be used as the validation set (12 days)\n",
    "\n",
    "3394 data/target examples will be used as the testing set (12 days)\n",
    "\n",
    "The shape for each prediction / target hour example is  (12, 307, 3)  , As the data will be loaded in batches of size 32\n",
    "\n",
    "its shape will be (32, 12, 307, 3)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727250008709,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "4KqeAbz1PxNz",
    "papermill": {
     "duration": 0.097144,
     "end_time": "2022-01-02T19:39:36.902795",
     "exception": false,
     "start_time": "2022-01-02T19:39:36.805651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "加载图信号数据，并将其转换为 PyTorch 的 DataLoader\n",
    "流程：\n",
    "1.转换数据类型：将 NumPy 数组转换为 PyTorch 张量，并确保它们的数据类型为浮点数。\n",
    "2.移动到设备：将张量移动到指定的设备（如 GPU）。\n",
    "3.创建数据集：使用 TensorDataset 将输入张量和目标张量配对，创建一个数据集对象。\n",
    "4.创建数据加载器：使用 DataLoader 将数据集分成小批量，并设置是否打乱数据。\n",
    "'''\n",
    "def load_graphdata_channel1(graph_signal_matrix_filename, num_of_hours, num_of_days, num_of_weeks, batch_size,\n",
    "                            shuffle=True, DEVICE = torch.device('cuda:0')):\n",
    "    '''\n",
    "    :param graph_signal_matrix_filename: str\n",
    "    :param num_of_hours: int\n",
    "    :param num_of_days: int\n",
    "    :param num_of_weeks: int\n",
    "    :param DEVICE:\n",
    "    :param batch_size: int\n",
    "    shuffle：是否在训练数据加载器中打乱数据。\n",
    "\n",
    "    :return:\n",
    "    three DataLoaders, each dataloader contains:\n",
    "    test_x_tensor: (B, N_nodes, in_feature, T_input)\n",
    "    test_decoder_input_tensor: (B, N_nodes, T_output)\n",
    "    test_target_tensor: (B, N_nodes, T_output)\n",
    "    '''\n",
    "\n",
    "    file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "    current_directory = os.getcwd()\n",
    "    print(\"当前工作目录:\", current_directory)\n",
    "    filename = os.path.join('./data/pems-dataset/PEMS04/', file + '_r' + str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(num_of_weeks)) +'_astcgn'\n",
    "    print('load file:', filename)\n",
    "    file_data = np.load(filename + '.npz')\n",
    "\n",
    "    train_x = file_data['train_x']  # (10181, 307, 3, 12)\n",
    "    train_x = train_x[:, :, 0:1, :]\n",
    "    train_target = file_data['train_target']  # (10181, 307, 12)\n",
    "\n",
    "    val_x = file_data['val_x']\n",
    "    val_x = val_x[:, :, 0:1, :]\n",
    "    val_target = file_data['val_target']\n",
    "\n",
    "    test_x = file_data['test_x']\n",
    "    test_x = test_x[:, :, 0:1, :]\n",
    "    test_target = file_data['test_target']\n",
    "\n",
    "    mean = file_data['mean'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "    std = file_data['std'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "\n",
    "    # ------- train_loader -------\n",
    "    train_x_tensor = torch.from_numpy(train_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    train_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    '''\n",
    "    torch.from_numpy 将 NumPy 数组转换为 PyTorch 张量。\n",
    "    .type(torch.FloatTensor) 将张量的数据类型转换为 FloatTensor，即 32 位浮点数\n",
    "    .to(DEVICE) 将张量移动到指定的设备（如 GPU 或 CPU）\n",
    "    torch.utils.data.TensorDataset 创建一个数据集对象，该对象将输入张量和目标张量配对,返回一个TensorDataset对象\n",
    "    TensorDataset 是PyTorch中的数据集类，用于将多个张量组合成一个数据集。每个样本由输入张量和目标张量组成。\n",
    "    torch.utils.data.DataLoader 创建一个数据加载器，用于批量加载数据\n",
    "    DataLoader 是PyTorch中的一个类，用于将数据集分成小批量，并在训练过程中方便地迭代数据。\n",
    "    '''\n",
    "\n",
    "\n",
    "    # ------- val_loader -------\n",
    "    val_x_tensor = torch.from_numpy(val_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    val_target_tensor = torch.from_numpy(val_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_x_tensor, val_target_tensor)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ------- test_loader -------\n",
    "    test_x_tensor = torch.from_numpy(test_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    test_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # print 打印数据尺寸\n",
    "    print('train:', train_x_tensor.size(), train_target_tensor.size())\n",
    "    print('val:', val_x_tensor.size(), val_target_tensor.size())\n",
    "    print('test:', test_x_tensor.size(), test_target_tensor.size())\n",
    "\n",
    "    return train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, mean, std\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 更改当前工作目录\n",
    "os.chdir(\"/content/drive/MyDrive/ColabNotebooks/pytorch_geometric_temporal/notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21321,
     "status": "ok",
     "timestamp": 1727250030635,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "nhpQTfJPPxN0",
    "outputId": "6b048274-8bfd-4fd2-e289-8ed98a860888",
    "papermill": {
     "duration": 45.287987,
     "end_time": "2022-01-02T19:40:22.221921",
     "exception": false,
     "start_time": "2022-01-02T19:39:36.933934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "graph_signal_matrix_filename = './data/pems-dataset/PEMS04/PEMS04.npz'\n",
    "batch_size = 32\n",
    "num_of_weeks = 0\n",
    "num_of_days = 0\n",
    "num_of_hours = 1\n",
    "\n",
    "train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, _mean, _std = load_graphdata_channel1(\n",
    "    graph_signal_matrix_filename, num_of_hours, num_of_days, num_of_weeks, batch_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBPXPFPiPxN0",
    "papermill": {
     "duration": 0.020698,
     "end_time": "2022-01-02T19:40:22.263490",
     "exception": false,
     "start_time": "2022-01-02T19:40:22.242792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading the graph ( The spatial part)\n",
    "\n",
    "In our case the graph is the Traffic Networks (literally, the network between the detectors (sensors) applied on the traffic networks\n",
    "\n",
    "In our example we have 307 detectors that when connected spatially, give our traffic network under investigation.\n",
    "\n",
    "# 加载图表（空间部分）\n",
    "\n",
    "在我们的例子中，该图是交通网络（字面意思是交通网络上应用的检测器（传感器）之间的网络\n",
    "\n",
    "在我们的示例中，我们有 307 个探测器，当它们在空间上连接时，可以对我们的交通网络进行调查。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBQsL5xCIIzR"
   },
   "source": [
    "加载图表（空间部分）\n",
    "\n",
    "在我们的例子中，该图是交通网络（字面意思是交通网络上应用的检测器（传感器）之间的网络\n",
    "\n",
    "在我们的示例中，我们有 307 个探测器，当它们在空间上连接时，可以对我们的交通网络进行调查。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1727250237275,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "9EM_4QPuIt_0"
   },
   "source": [
    "def get_adjacency_matrix(distance_df_filename, num_of_vertices, id_filename=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_df_filename: str, path of the csv file contains edges information\n",
    "    num_of_vertices: int, the number of vertices\n",
    "    Returns\n",
    "    ----------\n",
    "    A: np.ndarray, adjacency matrix\n",
    "    '''\n",
    "    if 'npy' in distance_df_filename:  # false\n",
    "        adj_mx = np.load(distance_df_filename)\n",
    "        return adj_mx, None\n",
    "    else:\n",
    "\n",
    "        #--------------------------------------------- read from here\n",
    "        import csv\n",
    "        A = np.zeros((int(num_of_vertices), int(num_of_vertices)),dtype=np.float32)\n",
    "        distaneA = np.zeros((int(num_of_vertices), int(num_of_vertices)), dtype=np.float32)\n",
    "\n",
    "        #------------ Ignore\n",
    "        if id_filename: # false\n",
    "            with open(id_filename, 'r') as f:\n",
    "                id_dict = {int(i): idx for idx, i in enumerate(f.read().strip().split('\\n'))}  # 把节点id（idx）映射成从0开始的索引\n",
    "\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[id_dict[i], id_dict[j]] = 1\n",
    "                    distaneA[id_dict[i], id_dict[j]] = distance\n",
    "            return A, distaneA\n",
    "\n",
    "        else:\n",
    "         #-------------Continue reading\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[i, j] = 1\n",
    "                    distaneA[i, j] = distance\n",
    "            return A, distaneA"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "executionInfo": {
     "elapsed": 2158,
     "status": "ok",
     "timestamp": 1727250239430,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "3Ag2uiv6PxN1",
    "outputId": "c8a93aa0-7731-4c85-ca2e-0c10e910a3a9",
    "papermill": {
     "duration": 0.404466,
     "end_time": "2022-01-02T19:40:22.750585",
     "exception": false,
     "start_time": "2022-01-02T19:40:22.346119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "id_filename = None\n",
    "# 邻接矩阵文件的路径，即存储交通网络连接信息的文件。\n",
    "adj_filename = './data/pems-dataset/PEMS04/PEMS04.csv'\n",
    "# num_of_vertices：节点的数量，即传感器的数量\n",
    "num_of_vertices = 307\n",
    "# get_adjacency_matrix：用于读取邻接矩阵文件并生成邻接矩阵adj_mx和距离矩阵distance_mx。邻接矩阵adj_mx是一个307x307的矩阵，表示节点之间的连接关系。\n",
    "adj_mx, distance_mx = get_adjacency_matrix(adj_filename, num_of_vertices, id_filename) #  adj_mx and distance_mx (307, 307)\n",
    "\n",
    "# 使用 networkx 库创建图并绘制出来。\n",
    "# 找到邻接矩阵中值为1的位置，这些位置表示节点之间的连接。\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "# 将行和列索引组合成边的列表。\n",
    "edges = zip(rows.tolist(), cols.tolist())\n",
    "gr = nx.Graph()\n",
    "gr.add_edges_from(edges)\n",
    "nx.draw(gr, node_size=3)\n",
    "plt.show()\n",
    "\n",
    "# 创建PyTorch张量表示边的索引\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "edges = zip(rows.tolist(), cols.tolist()) # 将行和列索引组合成边的列表。\n",
    "# torch.LongTensor(np.array([rows, cols]))：将边的行和列索引转换为PyTorch的长整型张量。\n",
    "edge_index_data = torch.LongTensor(np.array([rows, cols])).to(DEVICE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvsy5I1UPxN1",
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.023484,
     "end_time": "2022-01-02T19:40:22.799782",
     "exception": false,
     "start_time": "2022-01-02T19:40:22.776298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Making the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d_taDn3PxN1",
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.022939,
     "end_time": "2022-01-02T19:40:22.846081",
     "exception": false,
     "start_time": "2022-01-02T19:40:22.823142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOXbOtZPPxN1",
    "papermill": {
     "duration": 0.021699,
     "end_time": "2022-01-02T19:40:22.890144",
     "exception": false,
     "start_time": "2022-01-02T19:40:22.868445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Temporal attention layer\n",
    "\n",
    "In the temporal dimension, there exist correlations between the traffic conditions in different time slices, and the correlations are also varying under different situations. Likewise, we use an attention mechanism to adaptively attach different importance to data.\n",
    "\n",
    "在时间维度上，不同时间片内的交通状况之间存在相关性，且不同情况下相关性也不同。同样，我们使用注意力机制来自适应地对数据赋予不同的重要性\n",
    "\n",
    "<img src=\"https://i.ibb.co/KwXCqJx/temp-attention.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0puHGk6PPxN1",
    "papermill": {
     "duration": 0.021848,
     "end_time": "2022-01-02T19:40:22.935135",
     "exception": false,
     "start_time": "2022-01-02T19:40:22.913287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To understand the equation :\n",
    "\n",
    "It learns to attend (focus) on which part of the time segement used as input. In our case we have 12 time points So it will generate 12 by 12 weights.\n",
    "\n",
    "要理解这个方程：\n",
    "\n",
    "它学习关注（关注）时间段的哪一部分用作输入。在我们的例子中，我们有 12 个时间点，因此它将生成 12 x 12 的权重。\n",
    "\n",
    "<img src=\"https://i.ibb.co/NZ4fh4k/atten2.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOzGOD_YPxN2",
    "papermill": {
     "duration": 0.0216,
     "end_time": "2022-01-02T19:40:22.978912",
     "exception": false,
     "start_time": "2022-01-02T19:40:22.957312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Spatial attention layer\n",
    "\n",
    "In the spatial dimension, the traffic conditions of different locations have influence among each other and the mutual influence is highly dynamic. Here, we use an attention mechanism (Feng et al. 2017) to adaptively capture the dynamic correlations between nodes in the spatial dimension.\n",
    "\n",
    "空间注意力层\n",
    "\n",
    "在空间维度上，不同地点的交通状况相互影响，且相互影响是高度动态的。在这里，我们使用注意力机制（Feng et al. 2017）来自适应捕获空间维度中节点之间的动态相关性。\n",
    "\n",
    "<img src=\"https://i.ibb.co/PGnj4MR/spatial1.png\" width=\"400\">\n",
    "\n",
    "<img src=\"https://i.ibb.co/G5jkKvr/spatial2.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0myhes4WPxN2",
    "papermill": {
     "duration": 0.022913,
     "end_time": "2022-01-02T19:40:23.023944",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.001031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The same as with the temporal attention; however, here the attention weights will be used inside a Graph convolution layer\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/stTfTFM/spat2.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWNO1MMPPxN2",
    "papermill": {
     "duration": 0.02208,
     "end_time": "2022-01-02T19:40:23.068302",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.046222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Spectral graph analysis on the spatial part\n",
    "Since the spatial part is represented as a graph, we will apply graph convolution to aggregate messages from neighbor nodes. The type of graph convolution that we are going to use is spectral convolution.\n",
    "\n",
    "* In spectral graph analysis, a graph is represented by its corresponding Laplacian matrix.\n",
    "* The properties of the graph structure can be obtained by analyzing Laplacian matrix and its eigenvalues\n",
    "\n",
    "* Laplacian matrix of a graph is defined as L = D − A,\n",
    "\n",
    "* Its normalized form is L = I − ((1/ sqrt(D) A ( 1/ sqrt(D))  \n",
    "\n",
    "where A is the adjacent matrix, I is a unit matrix, and the degree matrix D (diagnoal diagonal matrix, consisting of node degrees,at the diagonal)\n",
    "\n",
    "The eigenvalue decomposition of the Laplacian matrix is L = U*Λ*(U.transpose()) , where Λ = diag([λ0, ..., λN −1]) is a diagonal matrix, and U is Fourier basis.\n",
    "\n",
    "U is an orthogonal matrix.\n",
    "\n",
    "The graph convolution is a convolution operation implemented by using linear operators that diagonalize in the Fourier domain to replace the classical convolution operator.\n",
    "\n",
    "However, it is expensive to directly perform the eigenvalue decomposition on the Laplacian matrix when the scale of the graph is large. Therefore, Chebyshev polynomials are adopted to solve this problem approximately but efficiently.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2SBUHSSPxN2",
    "papermill": {
     "duration": 0.022314,
     "end_time": "2022-01-02T19:40:23.157536",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.135222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The ASTGCN model structure\n",
    "\n",
    "\n",
    "The model is composed of two ASTGCN blocks followed by a final layer\n",
    "\n",
    "Original x (input) is (32, 307, 1, 12) -Block1> (32, 307, 64, 12) -Block2> (32, 307, 64, 12) -permute-> (32, 12, 307,64)\n",
    "            # -final_conv-> (32, 12, 307, 1) -reshape-> (32,307,12) \"The target\"\n",
    "            \n",
    "该模型是具有相同结构的三个独立组件的融合，旨在分别对历史数据的近期、日周期和周周期依赖性进行建模。这在之前的笔记本中已经讨论过\n",
    "\n",
    "The model is  the fusion of three independent components with the same structure, which are designed to respectively model the recent, daily-periodic and weekly-periodic dependencies of the historical data. This is discussed in the previous notebook (https://www.kaggle.com/elmahy/processing-traffic-data-for-deep-learning-projects).\n",
    "\n",
    "But in our case we will only focus on the recent segment (last hour segment) i.e. X_h  \n",
    "\n",
    "![astgcn.png](attachment:ec80c00c-6836-4449-85ba-e0b9d174a2f5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PMJ2lTVl-i5"
   },
   "source": [
    "【debug】/usr/local/lib/python3.10/dist-packages/torch_geometric_temporal/nn/attention/tsagcn.py\n",
    "\n",
    "删除：from torch_geometric.utils.to_dense_adj import to_dense_adj\n",
    "\n",
    "增加：from torch_geometric.utils import to_dense_adj"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!sed -i 's/from torch_geometric.utils.to_dense_adj import to_dense_adj/from torch_geometric.utils import to_dense_adj/' /usr/local/lib/python3.10/dist-packages/torch_geometric_temporal/nn/attention/tsagcn.py"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1727250239431,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "cXfSWjwkPxN2",
    "papermill": {
     "duration": 0.04099,
     "end_time": "2022-01-02T19:40:23.220618",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.179628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from torch_geometric_temporal.nn.attention import ASTGCN   # For information about the architecture check the source code"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFQ9CmScPxN2",
    "papermill": {
     "duration": 0.021947,
     "end_time": "2022-01-02T19:40:23.264474",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.242527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialized the ASTGCN model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1727250239431,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "G6w8fbiJPxN2",
    "outputId": "c8fc09a1-1982-4bc8-c23b-a1abe7fbd16c",
    "papermill": {
     "duration": 0.066646,
     "end_time": "2022-01-02T19:40:23.353254",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.286608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "nb_block = 2\n",
    "in_channels = 1\n",
    "K = 3\n",
    "nb_chev_filter = 64\n",
    "nb_time_filter = 64\n",
    "time_strides = num_of_hours\n",
    "num_for_predict = 12\n",
    "len_input = 12\n",
    "#L_tilde = scaled_Laplacian(adj_mx)\n",
    "#cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(DEVICE) for i in cheb_polynomial(L_tilde, K)]\n",
    "net = ASTGCN( nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, num_for_predict, len_input, num_of_vertices).to(DEVICE)\n",
    "\n",
    "print(net)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1727250239431,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "EXRnQnIXBchf",
    "outputId": "325d6982-5969-45e7-e603-0ddf8b5e0c35"
   },
   "source": [
    "# 打印模型的所有参数及其名称\n",
    "for name, param in net.named_parameters():\n",
    "    print(f\"Layer: {name} | Shape: {param.shape} | Requires Grad: {param.requires_grad}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1727250239431,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "9tZRtCZdPxN2",
    "outputId": "4da0ad80-3925-4df0-a555-85fb9f4e4777",
    "papermill": {
     "duration": 0.101833,
     "end_time": "2022-01-02T19:40:23.477478",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.375645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#------------------------------------------------------\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "print('Net\\'s state_dict:')\n",
    "total_param = 0\n",
    "'''\n",
    "state_dict() 方法返回一个字典，其中包含模型的所有参数和缓冲区\n",
    "param_tensor 是参数的名称。\n",
    "param_size 是参数张量的大小。\n",
    "param_device 是参数张量所在的设备。\n",
    "np.prod(param_size) 计算参数张量中所有元素的乘积，即参数的总数。\n",
    "'''\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, '\\t', net.state_dict()[param_tensor].size(), '\\t', net.state_dict()[param_tensor].device)\n",
    "    total_param += np.prod(net.state_dict()[param_tensor].size())\n",
    "print('Net\\'s total params:', total_param)\n",
    "#--------------------------------------------------\n",
    "print('Optimizer\\'s state_dict:')\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, '\\t', optimizer.state_dict()[var_name])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRcapoVcPxN3",
    "papermill": {
     "duration": 0.022902,
     "end_time": "2022-01-02T19:40:23.523502",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.500600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Defining the loss function\n",
    "\n",
    " 1. masked_mae 处理缺失值或某些特定值（如填充值）时非常有用。函数通过掩码来忽略这些值，从而计算有效的 MAE。\n",
    "\n",
    " 平均绝对误差（MAE）\n",
    "\n",
    "\n",
    "torch.where(condition, x, y)\n",
    "\n",
    "    condition: 一个布尔张量，表示条件。\n",
    "\n",
    "    x: 当条件为 True 时选取的值。\n",
    "\n",
    "    y: 当条件为 False 时选取的值。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1037,
     "status": "ok",
     "timestamp": 1727250525910,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "ItduUiMXPxN3",
    "papermill": {
     "duration": 0.032533,
     "end_time": "2022-01-02T19:40:23.578674",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.546141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def masked_mae(preds, labels, null_val=np.nan):\n",
    "  '''\n",
    "  :param: preds（预测值）\n",
    "          labels（真实标签）\n",
    "          null_val（表示无效值的标记，默认是 NaN）\n",
    "  '''\n",
    "  if np.isnan(null_val):\n",
    "    # 生成一个掩码 mask，掩码中的每个元素表示 labels 中对应位置是否不是 NaN（即 True 表示不是 NaN，False 表示是 NaN）\n",
    "    # ~ 按位取反运算符\n",
    "      mask = ~torch.isnan(labels)\n",
    "  else:\n",
    "    # 生成一个掩码 mask，掩码中的每个元素表示 labels 中对应位置是否不等于 null_val\n",
    "      mask = (labels != null_val)\n",
    "  mask = mask.float()\n",
    "  mask /= torch.mean((mask))\n",
    "\n",
    "  mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "  # 计算绝对误差: 计算预测值 preds 和真实标签 labels 之间的绝对误差\n",
    "  loss = torch.abs(preds - labels)\n",
    "  # 应用掩码: 将绝对误差与掩码相乘，这样掩码为 0 的位置（即无效值位置）的误差将被忽略（置为 0）\n",
    "  loss = loss * mask\n",
    "  # 处理误差中的 NaN 值: 将误差中的 NaN 值替换为 0。这是为了防止后续计算中出现 NaN 值\n",
    "  loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "  # 返回带掩码的 MAE: 计算并返回误差的均值，即带掩码的平均绝对误差\n",
    "  return torch.mean(loss)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1727250526745,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "YACTFthbPxN3",
    "papermill": {
     "duration": 0.032116,
     "end_time": "2022-01-02T19:40:23.633813",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.601697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "masked_flag=0\n",
    "criterion = nn.L1Loss().to(DEVICE)\n",
    "criterion_masked = masked_mae\n",
    "loss_function = 'mse'\n",
    "\n",
    "metric_method = 'unmask'\n",
    "missing_value=0.0\n",
    "\n",
    "\n",
    "if loss_function=='masked_mse':\n",
    "    criterion_masked = masked_mse         #nn.MSELoss().to(DEVICE)\n",
    "    masked_flag=1\n",
    "elif loss_function=='masked_mae':\n",
    "    criterion_masked = masked_mae\n",
    "    masked_flag = 1\n",
    "elif loss_function == 'mae':\n",
    "    criterion = nn.L1Loss().to(DEVICE)\n",
    "    masked_flag = 0\n",
    "elif loss_function == 'rmse':\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    masked_flag= 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727250526745,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "HUD2RSTXPxN3",
    "papermill": {
     "duration": 0.036068,
     "end_time": "2022-01-02T19:40:23.692677",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.656609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def compute_val_loss_mstgcn(net, val_loader, criterion,  masked_flag,missing_value,sw, epoch, edge_index_data, limit=None):\n",
    "    '''\n",
    "    for rnn, compute mean loss on validation set\n",
    "    :param net: model\n",
    "    :param val_loader: torch.utils.data.utils.DataLoader\n",
    "    :param criterion: torch.nn.MSELoss\n",
    "    :param sw: tensorboardX.SummaryWriter\n",
    "    :param global_step: int, current global_step\n",
    "    :param limit: int,\n",
    "    :return: val_loss\n",
    "    '''\n",
    "    # 将模型设置为评估模式 net.eval()\n",
    "    net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loader_length = len(val_loader)  # nb of batch\n",
    "        tmp = []  # batch loss\n",
    "        for batch_index, batch_data in enumerate(val_loader):\n",
    "            encoder_inputs, labels = batch_data\n",
    "            outputs = net(encoder_inputs, edge_index_data)\n",
    "            # 根据 masked_flag 来选择合适的损失函数。如果 masked_flag 为真，则使用带掩码的损失函数，否则使用标准损失函数。\n",
    "            if masked_flag:\n",
    "                loss = criterion(outputs, labels, missing_value)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            tmp.append(loss.item())\n",
    "            if batch_index % 100 == 0:\n",
    "                print('validation batch %s / %s, loss: %.2f' % (batch_index + 1, val_loader_length, loss.item()))\n",
    "            if (limit is not None) and batch_index >= limit:\n",
    "                break\n",
    "\n",
    "        validation_loss = sum(tmp) / len(tmp)\n",
    "        sw.add_scalar('validation_loss', validation_loss, epoch)\n",
    "    return validation_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727250526745,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "pMMfjwiLPxN3",
    "papermill": {
     "duration": 0.029214,
     "end_time": "2022-01-02T19:40:23.744592",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.715378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "global_step = 0\n",
    "best_epoch = 0\n",
    "# np.inf 表示正无穷大\n",
    "best_val_loss = np.inf\n",
    "start_time= time()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1727252608749,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "jgBaRYKDRyMr"
   },
   "source": [
    "# train model\n",
    "def train_model():\n",
    "  for epoch in range(20):\n",
    "      params_filename = os.path.join('./data/pems-dataset/', 'epoch_%s.params' % epoch)\n",
    "      masked_flag = 1\n",
    "      if masked_flag:\n",
    "          val_loss = compute_val_loss_mstgcn(net, val_loader, criterion_masked, masked_flag,missing_value,sw, epoch,edge_index_data)\n",
    "      else:\n",
    "          val_loss = compute_val_loss_mstgcn(net, val_loader, criterion, masked_flag, missing_value, sw, epoch,edge_index_data)\n",
    "\n",
    "      if val_loss < best_val_loss:\n",
    "          best_val_loss = val_loss\n",
    "          best_epoch = epoch\n",
    "          torch.save(net.state_dict(), params_filename)\n",
    "          print('save parameters to file: %s' % params_filename)\n",
    "\n",
    "      # 训练模式\n",
    "      net.train()  # ensure dropout layers are in train mode\n",
    "\n",
    "      for batch_index, batch_data in enumerate(train_loader):\n",
    "          encoder_inputs, labels = batch_data   # encoder_inputs torch.Size([32, 307, 1, 12])  label torch.Size([32, 307, 12])\n",
    "          # 清除上一步的梯度。\n",
    "          optimizer.zero_grad()\n",
    "          # 前向传播计算模型输出\n",
    "          outputs = net(encoder_inputs, edge_index_data) # torch.Size([32, 307, 12])\n",
    "\n",
    "          if masked_flag:\n",
    "              loss = criterion_masked(outputs, labels,missing_value)\n",
    "          else :\n",
    "              loss = criterion(outputs, labels)\n",
    "\n",
    "          # 反向传播计算梯度\n",
    "          loss.backward()\n",
    "          #  更新模型参数\n",
    "          optimizer.step()\n",
    "          training_loss = loss.item()\n",
    "          global_step += 1\n",
    "          sw.add_scalar('training_loss', training_loss, global_step)\n",
    "\n",
    "          if global_step % 200 == 0:\n",
    "              print('global step: %s, training loss: %.2f, time: %.2fs' % (global_step, training_loss, time() - start_time))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1727252809881,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "cWW7zK5bPxN3",
    "outputId": "2a5efcd7-1c3b-48e0-ca7e-8152dc458635",
    "papermill": {
     "duration": 1126.571255,
     "end_time": "2022-01-02T19:59:10.338875",
     "exception": false,
     "start_time": "2022-01-02T19:40:23.767620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 检查是否存在已训练好的参数文件\n",
    "params_filename = os.path.join('./data/pems-dataset/', 'epoch_12.params')\n",
    "best_params_file = None\n",
    "if os.path.exists(params_filename):\n",
    "  best_params_file = params_filename\n",
    "  best_epoch = 12\n",
    "\n",
    "if best_params_file:\n",
    "  print(f'Found trained parameters file: {best_params_file}')\n",
    "  net.load_state_dict(torch.load(best_params_file))\n",
    "  print('Loaded best model parameters. Skipping training and entering evaluation mode.')\n",
    "else:\n",
    "  print('No trained model parameters file found. Starting training.')\n",
    "  train_model()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6948,
     "status": "ok",
     "timestamp": 1727252819283,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "2ZHN2_MzPxN3",
    "outputId": "3d1af0ec-d8e6-44c0-b9e0-265c77d2f6d9",
    "papermill": {
     "duration": 7.428189,
     "end_time": "2022-01-02T19:59:17.814238",
     "exception": false,
     "start_time": "2022-01-02T19:59:10.386049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# model.eval()\n",
    "net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_loader_length = len(test_loader)  # nb of batch\n",
    "    tmp = []  # batch loss\n",
    "    for batch_index, batch_data in enumerate(test_loader):\n",
    "        encoder_inputs, labels = batch_data\n",
    "        outputs = net(encoder_inputs, edge_index_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        tmp.append(loss.item())\n",
    "        if batch_index % 100 == 0:\n",
    "            print('test_loss batch %s / %s, loss: %.2f' % (batch_index + 1, test_loader_length, loss.item()))\n",
    "\n",
    "\n",
    "    test_loss = sum(tmp) / len(tmp)\n",
    "    sw.add_scalar('test_loss', test_loss, epoch)\n",
    "print(test_loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfFXCBGpPxN3",
    "papermill": {
     "duration": 0.046401,
     "end_time": "2022-01-02T19:59:17.907126",
     "exception": false,
     "start_time": "2022-01-02T19:59:17.860725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pickig a random time point and visualizing the predictions of the first 50 detectors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1727252875836,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "SWDCWQ9jPxN3",
    "outputId": "482840f9-2378-4074-810f-b13ef072d008",
    "papermill": {
     "duration": 0.055894,
     "end_time": "2022-01-02T19:59:18.008925",
     "exception": false,
     "start_time": "2022-01-02T19:59:17.953031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "sample_output = outputs[0]  # prediction\n",
    "sample_labels = labels[0] # truth\n",
    "print(sample_output.shape, sample_labels.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1727252928640,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "tR1w0LjLS7JZ",
    "outputId": "6af860f1-f510-45e2-99b8-8b0894229006"
   },
   "source": [
    "sample_labels[0][1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1727252877479,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     },
     "user_tz": -480
    },
    "id": "5B3nWqGcPxN4",
    "outputId": "7380c681-4e54-49d2-f232-895a2da2c059",
    "papermill": {
     "duration": 0.335176,
     "end_time": "2022-01-02T19:59:18.390710",
     "exception": false,
     "start_time": "2022-01-02T19:59:18.055534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(30,4), dpi=80)\n",
    "for i in range(50):\n",
    "    new_i = i * 12\n",
    "    plt.plot(range(0+new_i,12+new_i),sample_output[i].detach().cpu().numpy(), color = 'red')\n",
    "    plt.plot(range(0+new_i,12+new_i),sample_labels[i].cpu().numpy(), color='blue')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xOXbOtZPPxN1",
    "nOzGOD_YPxN2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "papermill": {
   "duration": 1234.454996,
   "end_time": "2022-01-02T19:59:18.744009",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-02T19:38:44.289013",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
