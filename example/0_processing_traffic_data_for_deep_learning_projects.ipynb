{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:44.301431Z",
     "iopub.status.busy": "2022-01-02T18:59:44.300782Z",
     "iopub.status.idle": "2022-01-02T18:59:44.303686Z",
     "shell.execute_reply": "2022-01-02T18:59:44.303074Z",
     "shell.execute_reply.started": "2021-12-10T11:52:23.421825Z"
    },
    "papermill": {
     "duration": 0.022357,
     "end_time": "2022-01-02T18:59:44.303801",
     "exception": false,
     "start_time": "2022-01-02T18:59:44.281444",
     "status": "completed"
    },
    "tags": [],
    "id": "M8__7HSeMQUg",
    "ExecuteTime": {
     "end_time": "2024-11-20T13:29:11.245896Z",
     "start_time": "2024-11-20T13:29:11.243686Z"
    }
   },
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011631,
     "end_time": "2022-01-02T18:59:44.327795",
     "exception": false,
     "start_time": "2022-01-02T18:59:44.316164",
     "status": "completed"
    },
    "tags": [],
    "id": "nN2OMm5yMQUi"
   },
   "source": [
    "## The original data\n",
    "\n",
    "We first read original data\n",
    "\n",
    "(The data used in this notebook have been uploaded here https://www.kaggle.com/elmahy/pems-dataset)\n",
    "\n",
    "Its shape is # (sequence_length, num_of_vertices, num_of_features) (16992, 307, 3)\n",
    "\n",
    "We have 307 detectors each detector detect three features such as speed of cars, congestion,etc.\n",
    "And we store that data every five minutes.\n",
    "In 5 mins, our data will be (1, 307, 3)\n",
    "In 1 hour ,  our data will be (12, 307, 3)\n",
    "In 1 day ,our data will be (12 * 24, 307, 3)\n",
    "In 59 days, our data will be (12 * 24 * 59, 307, 3) =  (16992, 307, 3)\n",
    "\n",
    "### .npz数据集记录了有关传感器站的地理信息。\n",
    "\n",
    "在我们的实验中考虑了三种流量测量，包括总流量、平均速度和平均占用率。\n",
    "\n",
    "这些测量值每 5 分钟记录一次，因此每小时有 12 个数据点，每天有 288 个数据点。在整个数据集中，每个传感器站有 16992 个数据点。\n",
    "\n",
    "### .csv from,to,cost 表示从一个节点到另一个节点的距离。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011628,
     "end_time": "2022-01-02T18:59:44.375398",
     "exception": false,
     "start_time": "2022-01-02T18:59:44.363770",
     "status": "completed"
    },
    "tags": [],
    "id": "g-_Mbb3MMQUk"
   },
   "source": [
    "\n",
    "\n",
    "In our example we sample data per hour, so ignore anything about weeks or days\n",
    "\n",
    "Each hour's data is used to predict the next hour's data\n",
    "\n",
    "the 1 hour data(INPUT) is of shape (12, 307, 3) and the next hour(TARGET) is the same  (12, 307, 3)\n",
    "\n",
    "The functions get_sample_indices and search_data simply finds the input and the target since the orininal data have 16992 points collected every 5 minutes\n",
    "\n",
    "\n",
    "for example,\n",
    "the first 12 points(0:12) will be extracted from original data have 16992 points i.e. original[0:12]\n",
    "The target is the next hour so it will  be the next 12 points(12:24) i.g. original[12:24]\n",
    "\n",
    "Cool ... generate more .. move 5 minutes in the future ....\n",
    "\n",
    "the first 12 points(1:13) will be extracted from original data have 16992 points i.e. original[1:13]\n",
    "The target is the next hour so it will  be the next 12 points(13:25) i.g. original[13:25]\n",
    "\n",
    "\n",
    "And sooooo on.\n",
    "\n",
    "How many INPUT data and TARGET examples do we have now ?\n",
    "We havee 16969 examples which is 16992 (original) - 23\n",
    "\n",
    "The missing 23 points are the first 11 points(first hour) in the original sequence and the last 12 points(last hour) in the sequence i.e. the first hour data can't be predicted from any other previous data and the last hour data can't be used for predicting any future data because it's the last one in the training. So we ingore both.\n",
    "\n",
    "## Data splitting\n",
    "\n",
    "10181 data/target examples will be used as the training set ( 35 days )\n",
    "\n",
    "3394 data/target examples will be used as the validation set (12 days)\n",
    "\n",
    "3394 data/target examples will be used as the testing set (12 days)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:44.410034Z",
     "iopub.status.busy": "2022-01-02T18:59:44.408973Z",
     "iopub.status.idle": "2022-01-02T18:59:44.411944Z",
     "shell.execute_reply": "2022-01-02T18:59:44.411362Z",
     "shell.execute_reply.started": "2021-12-10T12:55:07.465483Z"
    },
    "papermill": {
     "duration": 0.024602,
     "end_time": "2022-01-02T18:59:44.412066",
     "exception": false,
     "start_time": "2022-01-02T18:59:44.387464",
     "status": "completed"
    },
    "tags": [],
    "id": "ZHBkxI43MQUk",
    "ExecuteTime": {
     "end_time": "2024-11-20T13:29:12.621605Z",
     "start_time": "2024-11-20T13:29:12.618751Z"
    }
   },
   "source": [
    "# search_data 旨在从时间序列数据中提取一组索引对，用于预测模型的输入。它通过指定的历史依赖关系（num_of_depend）和其他参数来确定这些索引对。\n",
    "def search_data(sequence_length, num_of_depend, label_start_idx, num_for_predict, units, points_per_hour):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence_length: int, length of all history data\n",
    "    num_of_depend: int,\n",
    "    label_start_idx: int, the first index of predicting target\n",
    "    num_for_predict: int, the number of points will be predicted for each sample\n",
    "    units: int, week: 7 * 24, day: 24, recent(hour): 1\n",
    "    points_per_hour: int, number of points per hour, depends on data\n",
    "    Returns\n",
    "    ----------\n",
    "    list[(start_idx, end_idx)]\n",
    "    \"\"\"\n",
    "\n",
    "    if points_per_hour < 0:\n",
    "        raise ValueError(\"points_per_hour should be greater than 0!\")\n",
    "\n",
    "    if label_start_idx + num_for_predict > sequence_length:\n",
    "        return None\n",
    "\n",
    "    x_idx = []\n",
    "    for i in range(1, num_of_depend + 1):\n",
    "        start_idx = label_start_idx - points_per_hour * units * i\n",
    "        end_idx = start_idx + num_for_predict\n",
    "        if start_idx >= 0:\n",
    "            x_idx.append((start_idx, end_idx))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if len(x_idx) != num_of_depend:\n",
    "        return None\n",
    "    # 返回索引对列表的逆序版本。这样可以确保最早的依赖在列表的最前面。\n",
    "    # x_idx[::-1] 等价于 x_idx[None:None:-1]，表示从列表的末尾开始，以步长 -1 逐步向前，直到列表的开头。\n",
    "    return x_idx[::-1]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:44.446912Z",
     "iopub.status.busy": "2022-01-02T18:59:44.446205Z",
     "iopub.status.idle": "2022-01-02T18:59:44.453515Z",
     "shell.execute_reply": "2022-01-02T18:59:44.453962Z"
    },
    "papermill": {
     "duration": 0.030037,
     "end_time": "2022-01-02T18:59:44.454113",
     "exception": false,
     "start_time": "2022-01-02T18:59:44.424076",
     "status": "completed"
    },
    "tags": [],
    "id": "kvC0wz4hMQUl",
    "ExecuteTime": {
     "end_time": "2024-11-20T13:29:13.303677Z",
     "start_time": "2024-11-20T13:29:13.298955Z"
    }
   },
   "source": [
    "# get_sample_indices 旨在从时间序列数据中提取样本，用于预测模型的输入。它根据指定的周、天和小时的依赖关系，返回相应的样本和预测目标。\n",
    "def get_sample_indices(data_sequence, num_of_weeks, num_of_days, num_of_hours, label_start_idx, num_for_predict,\n",
    "                       points_per_hour=12):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_sequence: np.ndarray shape is (sequence_length, num_of_vertices, num_of_features)\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    label_start_idx: int, the first index of predicting target\n",
    "    num_for_predict: int,the number of points will be predicted for each sample\n",
    "    points_per_hour: int, default 12, number of points per hour\n",
    "    Returns\n",
    "    ----------\n",
    "    week_sample: np.ndarray shape is (num_of_weeks * points_per_hour, num_of_vertices, num_of_features)\n",
    "    day_sample: np.ndarray shape is (num_of_days * points_per_hour,  num_of_vertices, num_of_features)\n",
    "    hour_sample: np.ndarray   shape is (num_of_hours * points_per_hour, num_of_vertices, num_of_features)\n",
    "    target: np.ndarray shape is (num_for_predict, num_of_vertices, num_of_features)\n",
    "    '''\n",
    "    week_sample, day_sample, hour_sample = None, None, None\n",
    "    #------------------------------------Ignore\n",
    "    # 检查预测的结束索引是否超出了数据序列的长度。如果超出，返回 None。\n",
    "    if label_start_idx + num_for_predict > data_sequence.shape[0]:\n",
    "        return week_sample, day_sample, hour_sample, None\n",
    "\n",
    "    if num_of_hours > 0:\n",
    "        hour_indices = search_data(data_sequence.shape[0], num_of_hours, label_start_idx, num_for_predict, 1, points_per_hour)\n",
    "        if not hour_indices:\n",
    "            return None, None, None, None\n",
    "        hour_sample = np.concatenate([data_sequence[i: j] for i, j in hour_indices], axis=0)\n",
    "        print('hour_indices：', hour_indices)\n",
    "\n",
    "    if num_of_hours > 10:\n",
    "        return 1;\n",
    "    # 提取预测目标\n",
    "    target = data_sequence[label_start_idx: label_start_idx + num_for_predict]\n",
    "\n",
    "    return week_sample, day_sample, hour_sample, target"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q:  TensorDataset 是PyTorch中的数据集类，用于将多个张量组合成一个数据集。每个样本由输入张量和目标张量组成。 这里为什么要将数据集分成输入张量和目标张量？\n",
    "\n",
    "```\n",
    "training_set = [np.concatenate(i, axis=0)  for i in zip(*all_samples[:split_line1])] #[(B,N,F,Th),(B,N,Tpre),(B,1)]\n",
    "```\n",
    "A:\n",
    "**1. 监督学习的需求**\n",
    "\n",
    "监督学习任务的核心是学习从输入特征到目标标签的映射关系。为了实现这一点，模型需要一组输入特征（输入张量）和对应的目标标签（目标张量）来进行训练。输入张量提供了模型的输入数据，而目标张量提供了模型需要预测的目标。\n",
    "\n",
    "\n",
    "**2. 数据加载和批处理**\n",
    "\n",
    "将数据集分成输入张量和目标张量，可以方便地使用 PyTorch 的 DataLoader 类进行数据加载和批处理。DataLoader 可以自动将数据集分成小批量（mini-batches）进行训练，从而提高训练效率和稳定性。\n",
    "\n",
    "\n",
    "**3. 数据预处理和增强**\n",
    "\n",
    "在训练过程中，输入张量和目标张量可能需要不同的预处理和数据增强操作。例如，输入图像可能需要进行归一化、随机裁剪等操作，而目标标签则不需要这些操作。将它们分开处理可以简化数据预处理流程。\n",
    "\n",
    "\n",
    "**4. 模型评估和调试**\n",
    "\n",
    "在模型评估和调试过程中，分开处理输入张量和目标张量可以更容易地计算各种评估指标（如准确率、损失等），以及进行调试和错误分析。"
   ],
   "metadata": {
    "id": "vFZtvXQcrUY3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:44.482208Z",
     "iopub.status.busy": "2022-01-02T18:59:44.481452Z",
     "iopub.status.idle": "2022-01-02T18:59:44.499450Z",
     "shell.execute_reply": "2022-01-02T18:59:44.499922Z",
     "shell.execute_reply.started": "2021-12-10T12:39:46.078756Z"
    },
    "papermill": {
     "duration": 0.033739,
     "end_time": "2022-01-02T18:59:44.500082",
     "exception": false,
     "start_time": "2022-01-02T18:59:44.466343",
     "status": "completed"
    },
    "tags": [],
    "id": "6jLyhHAIMQUl",
    "ExecuteTime": {
     "end_time": "2024-11-20T13:38:08.960350Z",
     "start_time": "2024-11-20T13:38:08.955442Z"
    }
   },
   "source": [
    "# 从图信号矩阵文件中读取数据，并生成用于训练、验证和测试的数据集。\n",
    "def read_and_generate_dataset(graph_signal_matrix_filename, num_of_weeks, num_of_days, num_of_hours, num_for_predict,\n",
    "                              points_per_hour=12):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_signal_matrix_filename: str, path of graph signal matrix file\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    num_for_predict: int\n",
    "    points_per_hour: int, default 12, depends on data\n",
    "    Returns\n",
    "    ----------\n",
    "    feature: np.ndarray, shape is (num_of_samples, num_of_depend * points_per_hour, num_of_vertices, num_of_features)\n",
    "    target: np.ndarray, shape is (num_of_samples, num_of_vertices, num_for_predict)\n",
    "    '''\n",
    "    #--------------------------------- Read original data\n",
    "    data_seq = np.load(graph_signal_matrix_filename)['data']  # (sequence_length, num_of_vertices, num_of_features) (16992, 307, 3)\n",
    "\n",
    "    #---------------------------------\n",
    "    all_samples = []\n",
    "    print('data_seq.shape[0]', data_seq.shape[0]) # 16992\n",
    "    for idx in range(data_seq.shape[0]):\n",
    "        sample = get_sample_indices(data_seq, num_of_weeks, num_of_days, num_of_hours, idx, num_for_predict,points_per_hour)\n",
    "        if (sample[0] is None) and (sample[1] is None) and (sample[2] is None):\n",
    "            continue\n",
    "\n",
    "        week_sample, day_sample, hour_sample, target = sample  #  week_sample, day_sample are None because we are predicting per hour\n",
    "        print(target.shape)  # hour_sample and target (12, 307, 3)\n",
    "        sample = []  # [(week_sample),(day_sample),(hour_sample),target,time_sample]\n",
    "\n",
    "        if num_of_hours > 0:\n",
    "            hour_sample = np.expand_dims(hour_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(hour_sample)\n",
    "\n",
    "        # [:, :, 0, :] 第二个轴选择0个元素==>>切片操作后，形状将从 (1, N, F, T) 变为 (1, N, T)\n",
    "        target = np.expand_dims(target, axis=0).transpose((0, 2, 3, 1))[:, :, 0, :]  # (1,N,T)\n",
    "        sample.append(target)\n",
    "        # 创建一个包含当前索引的时间样本，并添加到 sample 列表中\n",
    "        time_sample = np.expand_dims(np.array([idx]), axis=0)  # (1,1)\n",
    "        print(time_sample)\n",
    "        sample.append(time_sample)\n",
    "        all_samples.append(\n",
    "            sample)  #sampe：[(week_sample),(day_sample),(hour_sample),target,time_sample] = [None, None, (1,N,F,Th),(1,N,Tpre),(1,1)]\n",
    "    print(all_samples[0])\n",
    "        \n",
    "    # 按比例将 all_samples 划分为训练集（60%）、验证集（20%）和测试集（20%）。使用 zip 和 np.concatenate 将样本拼接成一个数组。\n",
    "    split_line1 = int(len(all_samples) * 0.6)\n",
    "    split_line2 = int(len(all_samples) * 0.8)\n",
    "\n",
    "    # zip 函数用于将多个可迭代对象打包成一个元组的迭代器。*subset_samples 表示对 subset_samples 进行解包操作。\n",
    "    training_set = [np.concatenate(i, axis=0) for i in\n",
    "                    zip(*all_samples[:split_line1])]  #[(B,N,F,Tw),(B,N,F,Td),(B,N,F,Th),(B,N,Tpre),(B,1)]\n",
    "    validation_set = [np.concatenate(i, axis=0) for i in zip(*all_samples[split_line1: split_line2])]\n",
    "    testing_set = [np.concatenate(i, axis=0) for i in zip(*all_samples[split_line2:])]\n",
    "\n",
    "    \"\"\"\n",
    "    zipped_samples = zip(*subset_samples)\n",
    "    结果是一个迭代器，内容如下：\n",
    "    [\n",
    "      (array1_1, array1_2, ..., array1_10),\n",
    "      (array2_1, array2_2, ..., array2_10),\n",
    "      (array3_1, array3_2, ..., array3_10),\n",
    "      (array4_1, array4_2, ..., array4_10),\n",
    "      (array5_1, array5_2, ..., array5_10)\n",
    "    ]\n",
    "\n",
    "    np.concatenate 后 training_set 中每个数组的形状：(B * split_line1, N, F, T)\n",
    "    \"\"\"\n",
    "\n",
    "    return training_set, validation_set, testing_set"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/ColabNotebooks/pytorch_geometric_temporal/notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:44.527565Z",
     "iopub.status.busy": "2022-01-02T18:59:44.526797Z",
     "iopub.status.idle": "2022-01-02T18:59:46.159491Z",
     "shell.execute_reply": "2022-01-02T18:59:46.160177Z",
     "shell.execute_reply.started": "2021-12-10T12:39:46.98975Z"
    },
    "papermill": {
     "duration": 1.648016,
     "end_time": "2022-01-02T18:59:46.160324",
     "exception": false,
     "start_time": "2022-01-02T18:59:44.512308",
     "status": "completed"
    },
    "tags": [],
    "id": "4LHSIjPIMQUm",
    "outputId": "66daef53-3113-4a93-ecf7-c7a699ed80c2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726837367941,
     "user_tz": -480,
     "elapsed": 3497,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-11-20T13:37:51.543379Z",
     "start_time": "2024-11-20T13:37:51.267076Z"
    }
   },
   "source": [
    "graph_signal_matrix_filename = './data/pems-dataset/PEMS04/PEMS04.npz'\n",
    "data = np.load(graph_signal_matrix_filename)\n",
    "\n",
    "# 查看文件中的键\n",
    "print(\"Keys in the .npz file:\", data.files)\n",
    "# 查看每个数组的形状和数据类型\n",
    "for key in data.files:\n",
    "    array = data[key]\n",
    "    print(f\"Array name: {key}\")\n",
    "    print(f\"Shape: {array.shape}\")  # (16992, 307, 3) 16992=59天*24小时*12(每五分钟采集一次数据)，307为节点数量，3为特征数量\n",
    "    # 特征：交通流量，平均速度，平均占用率\n",
    "    print(f\"Data type: {array.dtype}\")\n",
    "    # print(array[:5])\n",
    "    print()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# flow = data['data'][:, 0, 0]  # (16992, 307)\n",
    "# print(flow.shape)\n",
    "# fig_flow = plt.figure(figsize=(15, 5))\n",
    "# plt.title('traffic Flow')\n",
    "# plt.xlabel('minute')\n",
    "# plt.ylabel('flow')\n",
    "# plt.plot(np.arange(len(flow)), flow, linestyle='-')\n",
    "# fig_flow.autofmt_xdate(rotation=45)\n",
    "# plt.show()\n",
    "# \n",
    "# speed = data['data'][:, 0, 1]  # (16992, 307)\n",
    "# print(speed.shape)\n",
    "# fig_speed = plt.figure(figsize=(15, 5))\n",
    "# plt.title('traffic Speed')\n",
    "# plt.xlabel('minute')\n",
    "# plt.ylabel('speed')\n",
    "# plt.plot(np.arange(len(speed)), speed, linestyle='-')\n",
    "# fig_speed.autofmt_xdate(rotation=45)\n",
    "# plt.show()\n",
    "# \n",
    "# occupy = data['data'][:, 0, 2]  # (16992, 307)\n",
    "# print(speed.shape)\n",
    "# fig_occupy = plt.figure(figsize=(15, 5))\n",
    "# plt.title('traffic Occupy')\n",
    "# plt.xlabel('minute')\n",
    "# plt.ylabel('Occupy')\n",
    "# plt.plot(np.arange(len(occupy)), occupy, linestyle='-')\n",
    "# fig_occupy.autofmt_xdate(rotation=45)\n",
    "# plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the .npz file: ['data']\n",
      "Array name: data\n",
      "Shape: (16992, 307, 3)\n",
      "Data type: float64\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:46.189644Z",
     "iopub.status.busy": "2022-01-02T18:59:46.188943Z",
     "iopub.status.idle": "2022-01-02T18:59:50.241384Z",
     "shell.execute_reply": "2022-01-02T18:59:50.241952Z",
     "shell.execute_reply.started": "2021-12-10T12:39:47.766013Z"
    },
    "papermill": {
     "duration": 4.068513,
     "end_time": "2022-01-02T18:59:50.242110",
     "exception": false,
     "start_time": "2022-01-02T18:59:46.173597",
     "status": "completed"
    },
    "tags": [],
    "id": "hc__a_wtMQUm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726838182255,
     "user_tz": -480,
     "elapsed": 6681,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     }
    },
    "outputId": "53a394ef-8c9d-4089-ae8c-ed78665e0031",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-20T13:39:54.019081Z"
    }
   },
   "source": [
    "data = np.load(graph_signal_matrix_filename)\n",
    "\n",
    "num_of_vertices = 307\n",
    "points_per_hour = 12\n",
    "num_for_predict = 12\n",
    "num_of_weeks = 0\n",
    "num_of_days = 0\n",
    "num_of_hours = 1\n",
    "\n",
    "training_set, validation_set, testing_set = read_and_generate_dataset(graph_signal_matrix_filename, 0, 0, num_of_hours, num_for_predict, points_per_hour=points_per_hour);"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_seq.shape[0] 16992\n",
      "hour_indices： [(0, 12)]\n",
      "(12, 307, 3)\n",
      "[[12]]\n",
      "hour_indices： [(1, 13)]\n",
      "(12, 307, 3)\n",
      "[[13]]\n",
      "hour_indices： [(2, 14)]\n",
      "(12, 307, 3)\n",
      "[[14]]\n",
      "hour_indices： [(3, 15)]\n",
      "(12, 307, 3)\n",
      "[[15]]\n",
      "hour_indices： [(4, 16)]\n",
      "(12, 307, 3)\n",
      "[[16]]\n",
      "hour_indices： [(5, 17)]\n",
      "(12, 307, 3)\n",
      "[[17]]\n",
      "hour_indices： [(6, 18)]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"training_set 的形状:\", [x.shape for x in training_set])\n",
    "print(training_set[0][0].shape)\n",
    "print([_x.shape for _x in training_set[:-2]])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vaCZekKkfKN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726840011447,
     "user_tz": -480,
     "elapsed": 420,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     }
    },
    "outputId": "d5d8f298-fce2-44a8-bebe-793e9c8013ea",
    "ExecuteTime": {
     "end_time": "2024-11-20T13:29:21.281217Z",
     "start_time": "2024-11-20T13:29:21.278951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set 的形状: [(10181, 307, 3, 12), (10181, 307, 12), (10181, 1)]\n",
      "(307, 3, 12)\n",
      "[(10181, 307, 3, 12)]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013082,
     "end_time": "2022-01-02T18:59:50.268271",
     "exception": false,
     "start_time": "2022-01-02T18:59:50.255189",
     "status": "completed"
    },
    "tags": [],
    "id": "NqaaOOIpMQUn"
   },
   "source": [
    "# Data normalization\n",
    "\n",
    " the data are transformed by zero-mean normalization x′ = x −mean(x) to let the average be 0.\n",
    "\n",
    " 零均值归一化（Zero-Mean Normalization）是一种常用的数据预处理方法，用于将数据的均值调整为0。具体操作是将每个数据点减去数据的均值。\n",
    "\n",
    " B 表示批量大小（Batch size）\n",
    " N 表示节点数量（Number of nodes）\n",
    " F 表示特征数量（Number of features）\n",
    " T 表示时间步长（Time steps）"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:50.299157Z",
     "iopub.status.busy": "2022-01-02T18:59:50.298455Z",
     "iopub.status.idle": "2022-01-02T18:59:50.307298Z",
     "shell.execute_reply": "2022-01-02T18:59:50.307778Z",
     "shell.execute_reply.started": "2021-12-10T13:03:05.96189Z"
    },
    "papermill": {
     "duration": 0.025887,
     "end_time": "2022-01-02T18:59:50.307951",
     "exception": false,
     "start_time": "2022-01-02T18:59:50.282064",
     "status": "completed"
    },
    "tags": [],
    "id": "hQfZJD0PMQUn",
    "ExecuteTime": {
     "end_time": "2024-11-20T13:29:22.985491Z",
     "start_time": "2024-11-20T13:29:22.981829Z"
    }
   },
   "source": [
    "def normalization(train, val, test):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train, val, test: np.ndarray (B,N,F,T)\n",
    "    Returns\n",
    "    ----------\n",
    "    stats: dict, two keys: mean and std 包含均值和标准差的字典\n",
    "    train_norm, val_norm,\n",
    "    test_norm: np.ndarray, shape is the same as original 归一化后的训练数据\n",
    "    '''\n",
    "\n",
    "    # 判断train 和 val的(N, F, T)形状；即在节点数量、特征数量和时间步长上的形状是相同的 train.shape[1:]=(307, 3, 12)\n",
    "    assert train.shape[1:] == val.shape[1:] and val.shape[1:] == test.shape[1:]\n",
    "\n",
    "    # mean：沿轴 (0, 1, 3) 计算均值，即对批量、节点和时间步进行平均，保留特征维度。\n",
    "    # std：沿轴 (0, 1, 3) 计算标准差，同样保留特征维度。\n",
    "    mean = train.mean(axis=(0, 1, 3), keepdims=True)  # train (B,N,F,T')\n",
    "    std = train.std(axis=(0, 1, 3), keepdims=True)\n",
    "    print('mean.shape:', mean.shape)\n",
    "    print('std.shape:', std.shape)\n",
    "\n",
    "    # 零均值和单位标准差归一化函数\n",
    "    def normalize(x):\n",
    "        return (x - mean) / std\n",
    "\n",
    "    # 归一化训练集、验证集和测试集\n",
    "    train_norm = normalize(train)\n",
    "    val_norm = normalize(val)\n",
    "    test_norm = normalize(test)\n",
    "\n",
    "    return {'_mean': mean, '_std': std}, train_norm, val_norm, test_norm"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:50.337613Z",
     "iopub.status.busy": "2022-01-02T18:59:50.336930Z",
     "iopub.status.idle": "2022-01-02T18:59:55.121356Z",
     "shell.execute_reply": "2022-01-02T18:59:55.120773Z"
    },
    "papermill": {
     "duration": 4.800485,
     "end_time": "2022-01-02T18:59:55.121487",
     "exception": false,
     "start_time": "2022-01-02T18:59:50.321002",
     "status": "completed"
    },
    "tags": [],
    "id": "VoFC1bB0MQUn",
    "outputId": "4472ac1d-ac6f-4798-a02a-8bf46cc9047e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726845777942,
     "user_tz": -480,
     "elapsed": 12464,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-11-20T13:29:26.118334Z",
     "start_time": "2024-11-20T13:29:23.628542Z"
    }
   },
   "source": [
    "# 从 training_set 中提取从第一个元素到倒数第三个元素（包括倒数第三个元素）的所有元素==>>在这里其实就是第一个元素(10181, 307, 3, 12)，然后使用 np.concatenate 函数沿着指定的轴（这里是 axis=-1，即T'轴）将这些元素连接起来。\n",
    "# training_set 的形状: [(10181, 307, 3, 12), (10181, 307, 12), (10181, 1)]\n",
    "train_x = np.concatenate(training_set[:-2], axis=-1)  # (B,N,F,T) (10181, 307, 3, 12)\n",
    "val_x = np.concatenate(validation_set[:-2], axis=-1)\n",
    "test_x = np.concatenate(testing_set[:-2], axis=-1)\n",
    "\n",
    "_train_x = training_set[0]\n",
    "assert np.array_equal(_train_x, train_x), \"Contents are not equal\"\n",
    "\n",
    "train_target = training_set[-2]  # (B,N,T) (10181, 307, 12)\n",
    "val_target = validation_set[-2]\n",
    "test_target = testing_set[-2]\n",
    "\n",
    "train_timestamp = training_set[-1]  # (B,1) (10181, 1)\n",
    "val_timestamp = validation_set[-1]\n",
    "test_timestamp = testing_set[-1]\n",
    "\n",
    "(stats, train_x_norm, val_x_norm, test_x_norm) = normalization(train_x, val_x, test_x)\n",
    "\n",
    "print('train_x_norm.shape:', train_x_norm.shape)\n",
    "\n",
    "all_data = {'train': {'x': train_x_norm, 'target': train_target, 'timestamp': train_timestamp},\n",
    "            'val': {'x': val_x_norm, 'target': val_target, 'timestamp': val_timestamp},\n",
    "            'test': {'x': test_x_norm, 'target': test_target, 'timestamp': test_timestamp},\n",
    "            'stats': {'_mean': stats['_mean'], '_std': stats['_std']}}"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean.shape: (1, 1, 3, 1)\n",
      "std.shape: (1, 1, 3, 1)\n",
      "train_x_norm.shape: (10181, 307, 3, 12)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:55.158986Z",
     "iopub.status.busy": "2022-01-02T18:59:55.158295Z",
     "iopub.status.idle": "2022-01-02T18:59:55.165069Z",
     "shell.execute_reply": "2022-01-02T18:59:55.166150Z",
     "shell.execute_reply.started": "2021-12-10T12:05:55.889468Z"
    },
    "papermill": {
     "duration": 0.031431,
     "end_time": "2022-01-02T18:59:55.166358",
     "exception": false,
     "start_time": "2022-01-02T18:59:55.134927",
     "status": "completed"
    },
    "tags": [],
    "id": "ERfAB5-eMQUo",
    "outputId": "64c44830-9151-4e0d-de11-c098a79f74b2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726845953759,
     "user_tz": -480,
     "elapsed": 673,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-11-20T13:29:26.128002Z",
     "start_time": "2024-11-20T13:29:26.124882Z"
    }
   },
   "source": [
    "print('train x:', all_data['train']['x'].shape)\n",
    "print('train target:', all_data['train']['target'].shape)\n",
    "print('train timestamp:', all_data['train']['timestamp'].shape)\n",
    "print()\n",
    "print('val x:', all_data['val']['x'].shape)\n",
    "print('val target:', all_data['val']['target'].shape)\n",
    "print('val timestamp:', all_data['val']['timestamp'].shape)\n",
    "print()\n",
    "print('test x:', all_data['test']['x'].shape)\n",
    "print('test target:', all_data['test']['target'].shape)\n",
    "print('test timestamp:', all_data['test']['timestamp'].shape)\n",
    "print()\n",
    "print('train data _mean :', all_data['stats']['_mean'].shape, '\\n', all_data['stats']['_mean'])\n",
    "print('train data _std :', all_data['stats']['_std'].shape, '\\n', all_data['stats']['_std'])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x: (10181, 307, 3, 12)\n",
      "train target: (10181, 307, 12)\n",
      "train timestamp: (10181, 1)\n",
      "\n",
      "val x: (3394, 307, 3, 12)\n",
      "val target: (3394, 307, 12)\n",
      "val timestamp: (3394, 1)\n",
      "\n",
      "test x: (3394, 307, 3, 12)\n",
      "test target: (3394, 307, 12)\n",
      "test timestamp: (3394, 1)\n",
      "\n",
      "train data _mean : (1, 1, 3, 1) \n",
      " [[[[2.07227338e+02]\n",
      "   [5.13195612e-02]\n",
      "   [6.34740574e+01]]]]\n",
      "train data _std : (1, 1, 3, 1) \n",
      " [[[[1.56477655e+02]\n",
      "   [4.78541626e-02]\n",
      "   [8.10351724e+00]]]]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "os.chdir('/content/drive/MyDrive/ColabNotebooks/pytorch_geometric_temporal/notebooks/data/pems-dataset/PEMS04')\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T18:59:55.204554Z",
     "iopub.status.busy": "2022-01-02T18:59:55.202501Z",
     "iopub.status.idle": "2022-01-02T19:00:42.325357Z",
     "shell.execute_reply": "2022-01-02T19:00:42.326407Z",
     "shell.execute_reply.started": "2021-12-10T12:05:57.459518Z"
    },
    "papermill": {
     "duration": 47.145799,
     "end_time": "2022-01-02T19:00:42.326581",
     "exception": false,
     "start_time": "2022-01-02T18:59:55.180782",
     "status": "completed"
    },
    "tags": [],
    "id": "2ueJGpA0MQUo",
    "outputId": "adba0b3c-ded9-401e-fbfa-47e5680f4896",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726846674562,
     "user_tz": -480,
     "elapsed": 63473,
     "user": {
      "displayName": "zheng haoxuan",
      "userId": "13442429265529431306"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-11-20T13:29:52.927551Z",
     "start_time": "2024-11-20T13:29:26.173845Z"
    }
   },
   "source": [
    "file = os.path.basename(graph_signal_matrix_filename).split('.')[0]  # 获取文件名 PEMS04\n",
    "dirpath = './data/pems-dataset/PEMS04/'\n",
    "filename = os.path.join(dirpath, file + '_r' + str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(\n",
    "    num_of_weeks)) + '_astcgn'\n",
    "print('save file:', filename)\n",
    "\"\"\"\n",
    "使用 np.savez_compressed 函数将数据保存到一个压缩的 .npz 文件中。\n",
    "filename 是保存文件的路径。\n",
    "传递多个关键字参数，将不同的数据保存到 .npz 文件中。每个关键字参数对应一个数组，具体包括：\n",
    "train_x：训练数据的输入特征。\n",
    "train_target：训练数据的目标值。\n",
    "train_timestamp：训练数据的时间戳。\n",
    "val_x：验证数据的输入特征。\n",
    "val_target：验证数据的目标值。\n",
    "val_timestamp：验证数据的时间戳。\n",
    "test_x：测试数据的输入特征。\n",
    "test_target：测试数据的目标值。\n",
    "test_timestamp：测试数据的时间戳。\n",
    "mean：数据的均值（用于数据标准化）。\n",
    "std：数据的标准差（用于数据标准化）。\n",
    "\"\"\"\n",
    "np.savez_compressed(filename,\n",
    "                    train_x=all_data['train']['x'], train_target=all_data['train']['target'],\n",
    "                    train_timestamp=all_data['train']['timestamp'],\n",
    "                    val_x=all_data['val']['x'], val_target=all_data['val']['target'],\n",
    "                    val_timestamp=all_data['val']['timestamp'],\n",
    "                    test_x=all_data['test']['x'], test_target=all_data['test']['target'],\n",
    "                    test_timestamp=all_data['test']['timestamp'],\n",
    "                    mean=all_data['stats']['_mean'], std=all_data['stats']['_std'])\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save file: ./data/pems-dataset/PEMS04/PEMS04_r1_d0_w0_astcgn\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "papermill": {
     "duration": 0.549297,
     "end_time": "2022-01-02T19:00:42.890522",
     "exception": false,
     "start_time": "2022-01-02T19:00:42.341225",
     "status": "completed"
    },
    "tags": [],
    "id": "vib0DzjnMQUo",
    "ExecuteTime": {
     "end_time": "2024-09-29T08:13:23.833156Z",
     "start_time": "2024-09-29T08:13:23.830547Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "duration": 63.803621,
   "end_time": "2022-01-02T19:00:43.947524",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-02T18:59:40.143903",
   "version": "2.1.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
