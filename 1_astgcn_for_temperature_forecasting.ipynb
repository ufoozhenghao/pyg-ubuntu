{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:34.737002Z",
     "start_time": "2024-11-06T01:24:33.316108Z"
    }
   },
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0')\n",
    "print(\"CUDA:\", USE_CUDA, DEVICE)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "sw = SummaryWriter(logdir='.', flush_secs=5)\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.typing import OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.transforms import LaplacianLambdaMax\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, get_laplacian\n",
    "from torch_geometric.utils import to_dense_adj"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True cuda:0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "728facee-37c1-4bc2-b6d0-33c2827015a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:34.744922Z",
     "start_time": "2024-11-06T01:24:34.740142Z"
    }
   },
   "source": [
    "'''\n",
    "加载图信号数据，并将其转换为 PyTorch 的 DataLoader\n",
    "流程：\n",
    "1.转换数据类型：将 NumPy 数组转换为 PyTorch 张量，并确保它们的数据类型为浮点数。\n",
    "2.移动到设备：将张量移动到指定的设备（如 GPU）。\n",
    "3.创建数据集：使用 TensorDataset 将输入张量和目标张量配对，创建一个数据集对象。\n",
    "4.创建数据加载器：使用 DataLoader 将数据集分成小批量，并设置是否打乱数据。\n",
    "'''\n",
    "def load_graphdata_channel1(graph_signal_matrix_filename, num_of_hours, batch_size,\n",
    "                            shuffle=True, DEVICE = torch.device('cuda:0')):\n",
    "    \"\"\"\n",
    "    :param graph_signal_matrix_filename: str\n",
    "    :param num_of_hours: int\n",
    "    :param num_of_days: int\n",
    "    :param num_of_weeks: int\n",
    "    :param DEVICE:\n",
    "    :param batch_size: int\n",
    "    shuffle：是否在训练数据加载器中打乱数据。\n",
    "\n",
    "    :return:\n",
    "    three DataLoaders, each dataloader contains:\n",
    "    test_x_tensor: (B, N_nodes, in_feature, T_input)\n",
    "    test_decoder_input_tensor: (B, N_nodes, T_output)\n",
    "    test_target_tensor: (B, N_nodes, T_output)\n",
    "    \"\"\"\n",
    "\n",
    "    # file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "    current_directory = os.getcwd()\n",
    "    # new_working_directory = '/media/nevo/DOCUMENT/OneDrive/100 大体积混凝土温控研究/双屿门大桥/code/ASTGCN-PYG/pytorch_geometric_temporal/notebooks/data/heat'\n",
    "    # os.chdir(new_working_directory)\n",
    "    # current_directory = os.getcwd()\n",
    "    print(\"当前工作目录:\", current_directory)\n",
    "    print('load file:', graph_signal_matrix_filename)\n",
    "    file_data = np.load(graph_signal_matrix_filename)\n",
    "\n",
    "    train_x = file_data['train_x']  # (10181, 307, 3, 12)\n",
    "    train_x = train_x[:, :, 0:1, :]\n",
    "    train_target = file_data['train_target']  # (10181, 307, 12)\n",
    "\n",
    "    val_x = file_data['val_x']\n",
    "    val_x = val_x[:, :, 0:1, :]\n",
    "    val_target = file_data['val_target']\n",
    "\n",
    "    test_x = file_data['test_x']\n",
    "    test_x = test_x[:, :, 0:1, :]\n",
    "    test_target = file_data['test_target']\n",
    "\n",
    "    mean = file_data['mean'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "    std = file_data['std'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "\n",
    "\n",
    "    # ------- train_loader -------\n",
    "    train_x_tensor = torch.from_numpy(train_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    train_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    '''\n",
    "    torch.from_numpy 将 NumPy 数组转换为 PyTorch 张量。\n",
    "    .type(torch.FloatTensor) 将张量的数据类型转换为 FloatTensor，即 32 位浮点数\n",
    "    .to(DEVICE) 将张量移动到指定的设备（如 GPU 或 CPU）\n",
    "    torch.utils.data.TensorDataset 创建一个数据集对象，该对象将输入张量和目标张量配对,返回一个TensorDataset对象\n",
    "    TensorDataset 是PyTorch中的数据集类，用于将多个张量组合成一个数据集。每个样本由输入张量和目标张量组成。\n",
    "    torch.utils.data.DataLoader 创建一个数据加载器，用于批量加载数据\n",
    "    DataLoader 是PyTorch中的一个类，用于将数据集分成小批量，并在训练过程中方便地迭代数据。\n",
    "    '''\n",
    "\n",
    "\n",
    "    # ------- val_loader -------\n",
    "    val_x_tensor = torch.from_numpy(val_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    val_target_tensor = torch.from_numpy(val_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_x_tensor, val_target_tensor)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ------- test_loader -------\n",
    "    test_x_tensor = torch.from_numpy(test_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    test_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # print 打印数据尺寸\n",
    "    print('train:', train_x_tensor.size(), train_target_tensor.size())\n",
    "    print('val:', val_x_tensor.size(), val_target_tensor.size())\n",
    "    print('test:', test_x_tensor.size(), test_target_tensor.size())\n",
    "\n",
    "    return train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, mean, std"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "cd1af1e2-dcd0-44d8-8520-0ad95aa118ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:34.889904Z",
     "start_time": "2024-11-06T01:24:34.787608Z"
    }
   },
   "source": [
    "graph_signal_matrix_filename = './data/62/62_quarter_single_dataset_astcgn.npz'\n",
    "batch_size = 16\n",
    "num_of_hours = 1\n",
    "\n",
    "train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, _mean, _std = load_graphdata_channel1(\n",
    "    graph_signal_matrix_filename, num_of_hours,batch_size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: /home/nevo/github_repo/pyg-ubuntu\n",
      "load file: ./data/62/62_quarter_single_dataset_astcgn.npz\n",
      "train: torch.Size([55, 62, 1, 15]) torch.Size([55, 62, 3])\n",
      "val: torch.Size([19, 62, 1, 15]) torch.Size([19, 62, 3])\n",
      "test: torch.Size([19, 62, 1, 15]) torch.Size([19, 62, 3])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "00976f32-67b4-455e-8679-b94b1ebaba82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:34.960869Z",
     "start_time": "2024-11-06T01:24:34.957508Z"
    }
   },
   "source": [
    "def get_adjacency_matrix(distance_df_filename, num_of_vertices, id_filename=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_df_filename: str, path of the csv file contains edges information\n",
    "    num_of_vertices: int, the number of vertices\n",
    "    Returns\n",
    "    ----------\n",
    "    A: np.ndarray, adjacency matrix\n",
    "    '''\n",
    "    if 'npy' in distance_df_filename:  # false\n",
    "        adj_mx = np.load(distance_df_filename)\n",
    "        return adj_mx, None\n",
    "    else:\n",
    "\n",
    "        #--------------------------------------------- read from here\n",
    "        import csv\n",
    "        A = np.zeros((int(num_of_vertices), int(num_of_vertices)),dtype=np.float32)\n",
    "        distaneA = np.zeros((int(num_of_vertices), int(num_of_vertices)), dtype=np.float32)\n",
    "\n",
    "        #------------ Ignore\n",
    "        if id_filename: # false\n",
    "            with open(id_filename, 'r') as f:\n",
    "                id_dict = {int(i): idx for idx, i in enumerate(f.read().strip().split('\\n'))}  # 把节点id（idx）映射成从0开始的索引\n",
    "\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[id_dict[i], id_dict[j]] = 1\n",
    "                    distaneA[id_dict[i], id_dict[j]] = distance\n",
    "            return A, distaneA\n",
    "\n",
    "        else:\n",
    "         #-------------Continue reading\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[i, j] = 1\n",
    "                    distaneA[i, j] = distance\n",
    "            return A, distaneA"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "854a2a4b-e6d8-443b-b4f1-9b2a0ffa3d6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:35.007070Z",
     "start_time": "2024-11-06T01:24:35.001983Z"
    }
   },
   "source": [
    "id_filename = './data/62/62_id_filename.csv'\n",
    "# 邻接矩阵文件的路径，即存储交通网络连接信息的文件。\n",
    "adj_filename = './data/62/62_node_distance.csv'\n",
    "# num_of_vertices：节点的数量，即传感器的数量\n",
    "num_of_vertices = 62    #62为混凝土内温度节点数量  总数量为66含进出水温节点\n",
    "# get_adjacency_matrix：用于读取邻接矩阵文件并生成邻接矩阵adj_mx和距离矩阵distance_mx。邻接矩阵adj_mx是一个66x66的矩阵，表示节点之间的连接关系。\n",
    "adj_mx, distance_mx = get_adjacency_matrix(adj_filename, num_of_vertices, id_filename)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用 networkx 库创建图并绘制出来。",
   "id": "c4897f9a8e852758"
  },
  {
   "cell_type": "code",
   "id": "ba3aea8f-ccab-4957-a5fe-0b21cf0f9dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:35.058060Z",
     "start_time": "2024-11-06T01:24:35.052790Z"
    }
   },
   "source": [
    "# 使用 networkx 库创建图并绘制出来。\n",
    "# 找到邻接矩阵中值为1的位置，这些位置表示节点之间的连接。\n",
    "# rows, cols = np.where(adj_mx == 1)\n",
    "# # 将行和列索引组合成边的列表。\n",
    "# edges = zip(rows.tolist(), cols.tolist())\n",
    "# gr = nx.Graph()\n",
    "# gr.add_edges_from(edges)\n",
    "# nx.draw(gr, node_size=3)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:35.113109Z",
     "start_time": "2024-11-06T01:24:35.105612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建PyTorch张量表示边的索引\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "edges = zip(rows.tolist(), cols.tolist()) # 将行和列索引组合成边的列表。\n",
    "# torch.LongTensor(np.array([rows, cols]))：将边的行和列索引转换为PyTorch的长整型张量。\n",
    "edge_index_data = torch.LongTensor(np.array([rows, cols])).to(DEVICE)"
   ],
   "id": "4bd123b1f809834c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:35.176185Z",
     "start_time": "2024-11-06T01:24:35.161121Z"
    }
   },
   "cell_type": "code",
   "source": "from torch_geometric_temporal.nn.attention import ASTGCN",
   "id": "a83e2eec367a5fef",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:35.244864Z",
     "start_time": "2024-11-06T01:24:35.208273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nb_block = 2\n",
    "in_channels = 1\n",
    "K = 3\n",
    "nb_chev_filter = 16\n",
    "nb_time_filter = 16\n",
    "# time_strides = num_of_hours\n",
    "time_strides = 1\n",
    "num_for_predict = 3\n",
    "len_input = 15\n",
    "#L_tilde = scaled_Laplacian(adj_mx)\n",
    "#cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(DEVICE) for i in cheb_polynomial(L_tilde, K)]\n",
    "net = ASTGCN( nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, num_for_predict, len_input, num_of_vertices).to(DEVICE)\n",
    "\n",
    "print(net)"
   ],
   "id": "c93041300b64c09e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTGCN(\n",
      "  (_blocklist): ModuleList(\n",
      "    (0): ASTGCNBlock(\n",
      "      (_temporal_attention): TemporalAttention()\n",
      "      (_spatial_attention): SpatialAttention()\n",
      "      (_chebconv_attention): ChebConvAttention(1, 16, K=3, normalization=None)\n",
      "      (_time_convolution): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
      "      (_residual_convolution): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): ASTGCNBlock(\n",
      "      (_temporal_attention): TemporalAttention()\n",
      "      (_spatial_attention): SpatialAttention()\n",
      "      (_chebconv_attention): ChebConvAttention(16, 16, K=3, normalization=None)\n",
      "      (_time_convolution): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
      "      (_residual_convolution): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_layer_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (_final_conv): Conv2d(15, 3, kernel_size=(1, 16), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:35.258589Z",
     "start_time": "2024-11-06T01:24:35.256706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印模型的所有参数及其名称\n",
    "for name, param in net.named_parameters():\n",
    "    print(f\"Layer: {name} | Shape: {param.shape} | Requires Grad: {param.requires_grad}\")"
   ],
   "id": "556a6ee2cd137967",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: _blocklist.0._temporal_attention._U1 | Shape: torch.Size([62]) | Requires Grad: True\n",
      "Layer: _blocklist.0._temporal_attention._U2 | Shape: torch.Size([1, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.0._temporal_attention._U3 | Shape: torch.Size([1]) | Requires Grad: True\n",
      "Layer: _blocklist.0._temporal_attention._be | Shape: torch.Size([1, 15, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.0._temporal_attention._Ve | Shape: torch.Size([15, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._W1 | Shape: torch.Size([15]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._W2 | Shape: torch.Size([1, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._W3 | Shape: torch.Size([1]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._bs | Shape: torch.Size([1, 62, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._Vs | Shape: torch.Size([62, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.0._chebconv_attention._weight | Shape: torch.Size([3, 1, 16]) | Requires Grad: True\n",
      "Layer: _blocklist.0._chebconv_attention._bias | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.0._time_convolution.weight | Shape: torch.Size([16, 16, 1, 3]) | Requires Grad: True\n",
      "Layer: _blocklist.0._time_convolution.bias | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.0._residual_convolution.weight | Shape: torch.Size([16, 1, 1, 1]) | Requires Grad: True\n",
      "Layer: _blocklist.0._residual_convolution.bias | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.0._layer_norm.weight | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.0._layer_norm.bias | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._U1 | Shape: torch.Size([62]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._U2 | Shape: torch.Size([16, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._U3 | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._be | Shape: torch.Size([1, 15, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._Ve | Shape: torch.Size([15, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._W1 | Shape: torch.Size([15]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._W2 | Shape: torch.Size([16, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._W3 | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._bs | Shape: torch.Size([1, 62, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._Vs | Shape: torch.Size([62, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.1._chebconv_attention._weight | Shape: torch.Size([3, 16, 16]) | Requires Grad: True\n",
      "Layer: _blocklist.1._chebconv_attention._bias | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.1._time_convolution.weight | Shape: torch.Size([16, 16, 1, 3]) | Requires Grad: True\n",
      "Layer: _blocklist.1._time_convolution.bias | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.1._residual_convolution.weight | Shape: torch.Size([16, 16, 1, 1]) | Requires Grad: True\n",
      "Layer: _blocklist.1._residual_convolution.bias | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.1._layer_norm.weight | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _blocklist.1._layer_norm.bias | Shape: torch.Size([16]) | Requires Grad: True\n",
      "Layer: _final_conv.weight | Shape: torch.Size([3, 15, 1, 16]) | Requires Grad: True\n",
      "Layer: _final_conv.bias | Shape: torch.Size([3]) | Requires Grad: True\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:35.315243Z",
     "start_time": "2024-11-06T01:24:35.302723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "print('Net\\'s state_dict:')\n",
    "total_param = 0\n",
    "'''\n",
    "state_dict() 方法返回一个字典，其中包含模型的所有参数和缓冲区\n",
    "param_tensor 是参数的名称。\n",
    "param_size 是参数张量的大小。\n",
    "param_device 是参数张量所在的设备。\n",
    "np.prod(param_size) 计算参数张量中所有元素的乘积，即参数的总数。\n",
    "'''\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, '\\t', net.state_dict()[param_tensor].size(), '\\t', net.state_dict()[param_tensor].device)\n",
    "    total_param += np.prod(net.state_dict()[param_tensor].size())\n",
    "print('Net\\'s total params:', total_param)\n",
    "#--------------------------------------------------\n",
    "print('Optimizer\\'s state_dict:')\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, '\\t', optimizer.state_dict()[var_name])"
   ],
   "id": "ff6e8dc7f85deee1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net's state_dict:\n",
      "_blocklist.0._temporal_attention._U1 \t torch.Size([62]) \t cuda:0\n",
      "_blocklist.0._temporal_attention._U2 \t torch.Size([1, 62]) \t cuda:0\n",
      "_blocklist.0._temporal_attention._U3 \t torch.Size([1]) \t cuda:0\n",
      "_blocklist.0._temporal_attention._be \t torch.Size([1, 15, 15]) \t cuda:0\n",
      "_blocklist.0._temporal_attention._Ve \t torch.Size([15, 15]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._W1 \t torch.Size([15]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._W2 \t torch.Size([1, 15]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._W3 \t torch.Size([1]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._bs \t torch.Size([1, 62, 62]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._Vs \t torch.Size([62, 62]) \t cuda:0\n",
      "_blocklist.0._chebconv_attention._weight \t torch.Size([3, 1, 16]) \t cuda:0\n",
      "_blocklist.0._chebconv_attention._bias \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.0._time_convolution.weight \t torch.Size([16, 16, 1, 3]) \t cuda:0\n",
      "_blocklist.0._time_convolution.bias \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.0._residual_convolution.weight \t torch.Size([16, 1, 1, 1]) \t cuda:0\n",
      "_blocklist.0._residual_convolution.bias \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.0._layer_norm.weight \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.0._layer_norm.bias \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._U1 \t torch.Size([62]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._U2 \t torch.Size([16, 62]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._U3 \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._be \t torch.Size([1, 15, 15]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._Ve \t torch.Size([15, 15]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._W1 \t torch.Size([15]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._W2 \t torch.Size([16, 15]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._W3 \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._bs \t torch.Size([1, 62, 62]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._Vs \t torch.Size([62, 62]) \t cuda:0\n",
      "_blocklist.1._chebconv_attention._weight \t torch.Size([3, 16, 16]) \t cuda:0\n",
      "_blocklist.1._chebconv_attention._bias \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.1._time_convolution.weight \t torch.Size([16, 16, 1, 3]) \t cuda:0\n",
      "_blocklist.1._time_convolution.bias \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.1._residual_convolution.weight \t torch.Size([16, 16, 1, 1]) \t cuda:0\n",
      "_blocklist.1._residual_convolution.bias \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.1._layer_norm.weight \t torch.Size([16]) \t cuda:0\n",
      "_blocklist.1._layer_norm.bias \t torch.Size([16]) \t cuda:0\n",
      "_final_conv.weight \t torch.Size([3, 15, 1, 16]) \t cuda:0\n",
      "_final_conv.bias \t torch.Size([3]) \t cuda:0\n",
      "Net's total params: 21280\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]}]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:35.356812Z",
     "start_time": "2024-11-06T01:24:35.350580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def masked_mae(preds, labels, null_val=np.nan):\n",
    "    '''\n",
    "    :param: preds（预测值）\n",
    "            labels（真实标签）\n",
    "            null_val（表示无效值的标记，默认是 NaN）\n",
    "    '''\n",
    "    if np.isnan(null_val):\n",
    "        # 生成一个掩码 mask，掩码中的每个元素表示 labels 中对应位置是否不是 NaN（即 True 表示不是 NaN，False 表示是 NaN）\n",
    "        # ~ 按位取反运算符\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        # 生成一个掩码 mask，掩码中的每个元素表示 labels 中对应位置是否不等于 null_val\n",
    "        mask = (labels != null_val)\n",
    "    mask = mask.float()\n",
    "    mask /= torch.mean((mask))\n",
    "\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    # 计算绝对误差: 计算预测值 preds 和真实标签 labels 之间的绝对误差\n",
    "\n",
    "    print('preds.shape:', preds.shape)\n",
    "    print('labels.shape:', labels.shape)\n",
    "\n",
    "    loss = torch.abs(preds - labels)\n",
    "    # 应用掩码: 将绝对误差与掩码相乘，这样掩码为 0 的位置（即无效值位置）的误差将被忽略（置为 0）\n",
    "    loss = loss * mask\n",
    "    # 处理误差中的 NaN 值: 将误差中的 NaN 值替换为 0。这是为了防止后续计算中出现 NaN 值\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    # 返回带掩码的 MAE: 计算并返回误差的均值，即带掩码的平均绝对误差\n",
    "    return torch.mean(loss)"
   ],
   "id": "e88604fa28faf102",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:24:35.410864Z",
     "start_time": "2024-11-06T01:24:35.402899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "masked_flag=0\n",
    "criterion = nn.L1Loss().to(DEVICE)\n",
    "criterion_masked = masked_mae\n",
    "loss_function = 'mse'\n",
    "\n",
    "metric_method = 'unmask'\n",
    "missing_value=0.0\n",
    "\n",
    "if loss_function=='masked_mse':\n",
    "    criterion_masked = masked_mse         #nn.MSELoss().to(DEVICE)\n",
    "    masked_flag=1\n",
    "elif loss_function=='masked_mae':\n",
    "    criterion_masked = masked_mae\n",
    "    masked_flag = 1\n",
    "elif loss_function == 'mae':\n",
    "    criterion = nn.L1Loss().to(DEVICE)\n",
    "    masked_flag = 0\n",
    "elif loss_function == 'rmse':\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    masked_flag= 0"
   ],
   "id": "6f187528d49ead9d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:55:38.581493Z",
     "start_time": "2024-11-06T01:55:38.571843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_val_loss_mstgcn(net, val_loader, criterion,  masked_flag,missing_value,sw, epoch, edge_index_data, limit=None):\n",
    "    '''\n",
    "    for rnn, compute mean loss on validation set\n",
    "    :param net: model\n",
    "    :param val_loader: torch.utils.data.utils.DataLoader\n",
    "    :param criterion: torch.nn.MSELoss\n",
    "    :param sw: tensorboardX.SummaryWriter\n",
    "    :param global_step: int, current global_step\n",
    "    :param limit: int,\n",
    "    :return: val_loss\n",
    "    '''\n",
    "    # 将模型设置为评估模式 net.eval()\n",
    "    net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loader_length = len(val_loader)  # nb of batch\n",
    "        tmp = []  # batch loss\n",
    "        for batch_index, batch_data in enumerate(val_loader):\n",
    "            encoder_inputs, labels = batch_data\n",
    "            outputs = net(encoder_inputs, edge_index_data)\n",
    "            # 根据 masked_flag 来选择合适的损失函数。如果 masked_flag 为真，则使用带掩码的损失函数，否则使用标准损失函数。\n",
    "            if masked_flag:\n",
    "                loss = criterion(outputs, labels, missing_value)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            tmp.append(loss.item())\n",
    "            if batch_index % 100 == 0:\n",
    "                print('validation batch %s / %s, loss: %.2f' % (batch_index + 1, val_loader_length, loss.item()))\n",
    "            if (limit is not None) and batch_index >= limit:\n",
    "                break\n",
    "\n",
    "        validation_loss = sum(tmp) / len(tmp)\n",
    "        sw.add_scalar('validation_loss', validation_loss, epoch)\n",
    "    return validation_loss"
   ],
   "id": "86bbdfc1071212fd",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:55:39.266464Z",
     "start_time": "2024-11-06T01:55:39.263567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "global_step = 0\n",
    "best_epoch = 0\n",
    "# np.inf 表示正无穷大\n",
    "best_val_loss = np.inf\n",
    "start_time= time()"
   ],
   "id": "aafbe0016c8c3192",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:55:43.059215Z",
     "start_time": "2024-11-06T01:55:40.079735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train model\n",
    "for epoch in range(20):\n",
    "    params_filename = os.path.join('./data/62/train_params/', 'epoch_%s.params' % epoch)\n",
    "    masked_flag = 1\n",
    "    if masked_flag:\n",
    "        val_loss = compute_val_loss_mstgcn(net, val_loader, criterion_masked, masked_flag,missing_value,sw, epoch,edge_index_data)\n",
    "    else:\n",
    "        val_loss = compute_val_loss_mstgcn(net, val_loader, criterion, masked_flag, missing_value, sw, epoch,edge_index_data)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(net.state_dict(), params_filename)\n",
    "        print('save parameters to file: %s' % params_filename)\n",
    "\n",
    "    # 训练模式\n",
    "    net.train()  # ensure dropout layers are in train mode\n",
    "\n",
    "    for batch_index, batch_data in enumerate(train_loader):\n",
    "        encoder_inputs, labels = batch_data   # encoder_inputs torch.Size([32, 307, 1, 12])  label torch.Size([32, 307, 12])\n",
    "        # 清除上一步的梯度。\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播计算模型输出\n",
    "        outputs = net(encoder_inputs, edge_index_data) # torch.Size([32, 307, 12])\n",
    "\n",
    "        if masked_flag:\n",
    "            loss = criterion_masked(outputs, labels,missing_value)\n",
    "        else :\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播计算梯度\n",
    "        loss.backward()\n",
    "        #  更新模型参数\n",
    "        optimizer.step()\n",
    "        training_loss = loss.item()\n",
    "        global_step += 1\n",
    "        sw.add_scalar('training_loss', training_loss, global_step)\n",
    "\n",
    "        if global_step % 200 == 0:\n",
    "            print('global step: %s, training loss: %.2f, time: %.2fs' % (global_step, training_loss, time() - start_time))"
   ],
   "id": "65dba54e02357c50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 5.55\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_0.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 5.44\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_1.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 5.34\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_2.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 5.25\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_3.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 5.18\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_4.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 5.11\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 5.05\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 4.66\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_7.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 4.38\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_8.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 3.73\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_9.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 2.76\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_10.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 1.88\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_11.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 1.41\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_12.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 1.31\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_13.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 0.95\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_14.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 1.27\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 0.88\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_16.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 0.91\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_17.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 0.80\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "save parameters to file: ./data/62/train_params/epoch_18.params\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "validation batch 1 / 2, loss: 0.93\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([3, 62, 3])\n",
      "labels.shape: torch.Size([3, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([16, 62, 3])\n",
      "labels.shape: torch.Size([16, 62, 3])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([7, 15, 62])\n",
      "RHS.shape: torch.Size([7, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "preds.shape: torch.Size([7, 62, 3])\n",
      "labels.shape: torch.Size([7, 62, 3])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:55:44.035925Z",
     "start_time": "2024-11-06T01:55:43.989597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_loader_length = len(test_loader)  # nb of batch\n",
    "    tmp = []  # batch loss\n",
    "    for batch_index, batch_data in enumerate(test_loader):\n",
    "        encoder_inputs, labels = batch_data\n",
    "        outputs = net(encoder_inputs, edge_index_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        tmp.append(loss.item())\n",
    "        if batch_index % 100 == 0:\n",
    "            print('test_loss batch %s / %s, loss: %.2f' % (batch_index + 1, test_loader_length, loss.item()))\n",
    "\n",
    "\n",
    "    test_loss = sum(tmp) / len(tmp)\n",
    "    sw.add_scalar('test_loss', test_loss, epoch)\n",
    "print(test_loss)"
   ],
   "id": "b1912e37c5103150",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([16, 15, 62])\n",
      "RHS.shape: torch.Size([16, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "test_loss batch 1 / 2, loss: 0.84\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "LHS.shape: torch.Size([3, 15, 62])\n",
      "RHS.shape: torch.Size([3, 62, 15])\n",
      "self._Ve.shape: torch.Size([15, 15])\n",
      "self._be.shape: torch.Size([1, 15, 15])\n",
      "0.8957428336143494\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pickig a random time point and visualizing the predictions of the first 50 detectors",
   "id": "a8131a468e81f7f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:59:02.222218Z",
     "start_time": "2024-11-06T01:59:02.209945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_output = outputs[0]  # prediction\n",
    "sample_labels = labels[0] # truth\n",
    "print(sample_output.shape, sample_labels.shape)\n",
    "sample_labels[0][1]"
   ],
   "id": "7dff9c5f00cd83fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 3]) torch.Size([62, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16.6600, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:59:53.684809Z",
     "start_time": "2024-11-06T01:59:53.586267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(30,4), dpi=80)\n",
    "for i in range(50):\n",
    "    new_i = i * 3   # 调整步长为 3，以匹配 y 数据的长度\n",
    "    plt.plot(range(0+new_i,3+new_i),sample_output[i].detach().cpu().numpy(), color = 'red')\n",
    "    plt.plot(range(0+new_i,3+new_i),sample_labels[i].cpu().numpy(), color='blue')\n",
    "plt.show()"
   ],
   "id": "ef3906e90f37da88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x320 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f2f38c44d032dba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
