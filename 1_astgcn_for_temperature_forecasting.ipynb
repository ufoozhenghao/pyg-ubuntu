{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T02:25:04.919710Z",
     "start_time": "2024-11-26T02:25:04.915190Z"
    }
   },
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0')\n",
    "print(\"CUDA:\", USE_CUDA, DEVICE)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "sw = SummaryWriter(logdir='.', flush_secs=5)\n",
    "\n",
    "# import math\n",
    "# from typing import Optional, List, Union\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.typing import OptTensor\n",
    "# from torch_geometric.nn.conv import MessagePassing\n",
    "# from torch_geometric.transforms import LaplacianLambdaMax\n",
    "# from torch_geometric.utils import remove_self_loops, add_self_loops, get_laplacian\n",
    "# from torch_geometric.utils import to_dense_adj"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True cuda:0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "728facee-37c1-4bc2-b6d0-33c2827015a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T03:10:59.992234Z",
     "start_time": "2024-11-26T03:10:59.973809Z"
    }
   },
   "source": [
    "'''\n",
    "加载图信号数据，并将其转换为 PyTorch 的 DataLoader\n",
    "流程：\n",
    "1.转换数据类型：将 NumPy 数组转换为 PyTorch 张量，并确保它们的数据类型为浮点数。\n",
    "2.移动到设备：将张量移动到指定的设备（如 GPU）。\n",
    "3.创建数据集：使用 TensorDataset 将输入张量和目标张量配对，创建一个数据集对象。\n",
    "4.创建数据加载器：使用 DataLoader 将数据集分成小批量，并设置是否打乱数据。\n",
    "'''\n",
    "\n",
    "\n",
    "def load_graphdata_channel1(graph_signal_matrix_filename, num_of_hours, batch_size,\n",
    "                            shuffle=True, DEVICE=torch.device('cuda:0')):\n",
    "    \"\"\"\n",
    "    :param graph_signal_matrix_filename: str\n",
    "    :param num_of_hours: int\n",
    "    :param num_of_days: int\n",
    "    :param num_of_weeks: int\n",
    "    :param DEVICE:\n",
    "    :param batch_size: int\n",
    "    shuffle：是否在训练数据加载器中打乱数据。\n",
    "\n",
    "    :return:\n",
    "    three DataLoaders, each dataloader contains:\n",
    "    test_x_tensor: (B, N_nodes, in_feature, T_input)\n",
    "    test_decoder_input_tensor: (B, N_nodes, T_output)\n",
    "    test_target_tensor: (B, N_nodes, T_output)\n",
    "    \"\"\"\n",
    "\n",
    "    print('load file:', graph_signal_matrix_filename)\n",
    "    file_data = np.load(graph_signal_matrix_filename, allow_pickle=True)\n",
    "    data = file_data['data'].item()\n",
    "\n",
    "    '''\n",
    "    torch.from_numpy 将 NumPy 数组转换为 PyTorch 张量。\n",
    "    .type(torch.FloatTensor) 将张量的数据类型转换为 FloatTensor，即 32 位浮点数\n",
    "    .to(DEVICE) 将张量移动到指定的设备（如 GPU 或 CPU）\n",
    "    torch.utils.data.TensorDataset 创建一个数据集对象，该对象将输入张量和目标张量配对,返回一个TensorDataset对象\n",
    "    TensorDataset 是PyTorch中的数据集类，用于将多个张量组合成一个数据集。每个样本由输入张量和目标张量组成。\n",
    "    torch.utils.data.DataLoader 创建一个数据加载器，用于批量加载数据\n",
    "    DataLoader 是PyTorch中的一个类，用于将数据集分成小批量，并在训练过程中方便地迭代数据。\n",
    "    '''\n",
    "\n",
    "    # ------- train_loader -------\n",
    "    # 将 NumPy 数组转换为 PyTorch 的 Tensor 并移动到指定设备\n",
    "    train_signal_0_tensor = torch.from_numpy(data['train']['signal_0']).type(torch.FloatTensor).to(DEVICE)\n",
    "    train_signal_1_tensor = torch.from_numpy(data['train']['signal_1']).type(torch.FloatTensor).to(DEVICE)\n",
    "    train_signal_2_tensor = torch.from_numpy(data['train']['signal_2']).type(torch.FloatTensor).to(DEVICE)\n",
    "    train_target_tensor = torch.from_numpy(data['train']['target']).type(torch.FloatTensor).to(DEVICE)\n",
    "    # 创建一个 TensorDataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_signal_0_tensor, train_signal_1_tensor, train_signal_2_tensor,\n",
    "                                                   train_target_tensor)\n",
    "    # 创建 DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # ------- val_loader -------\n",
    "    val_signal_0_tensor = torch.from_numpy(data['val']['signal_0']).type(torch.FloatTensor).to(DEVICE)\n",
    "    val_signal_1_tensor = torch.from_numpy(data['val']['signal_1']).type(torch.FloatTensor).to(DEVICE)\n",
    "    val_signal_2_tensor = torch.from_numpy(data['val']['signal_2']).type(torch.FloatTensor).to(DEVICE)\n",
    "    val_target_tensor = torch.from_numpy(data['val']['target']).type(torch.FloatTensor).to(DEVICE)\n",
    "\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_signal_0_tensor, val_signal_1_tensor, val_signal_2_tensor,\n",
    "                                                 val_target_tensor)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # ------- test_loader -------\n",
    "    test_signal_0_tensor = torch.from_numpy(data['test']['signal_0']).type(torch.FloatTensor).to(DEVICE)\n",
    "    test_signal_1_tensor = torch.from_numpy(data['test']['signal_1']).type(torch.FloatTensor).to(DEVICE)\n",
    "    test_signal_2_tensor = torch.from_numpy(data['test']['signal_2']).type(torch.FloatTensor).to(DEVICE)\n",
    "    test_target_tensor = torch.from_numpy(data['test']['target']).type(torch.FloatTensor).to(DEVICE)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_signal_0_tensor, test_signal_1_tensor, test_signal_2_tensor,\n",
    "                                                  test_target_tensor)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # mean std\n",
    "    stats_data = {}\n",
    "    for type_ in ['signal_0', 'signal_1', 'signal_2']:\n",
    "        stats = data['stats'][type_]\n",
    "        stats_data[type_ + '_mean'] = stats['_mean']\n",
    "        stats_data[type_ + '_std'] = stats['_std']\n",
    "\n",
    "    # print 打印数据尺寸\n",
    "    print('train:', train_signal_0_tensor.size(), train_target_tensor.size())\n",
    "    print('val:', val_signal_0_tensor.size(), val_target_tensor.size())\n",
    "    print('test:', test_signal_0_tensor.size(), test_target_tensor.size())\n",
    "    print('stats_data:', stats_data)\n",
    "\n",
    "    return train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, stats_data"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "cd1af1e2-dcd0-44d8-8520-0ad95aa118ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T03:11:00.975214Z",
     "start_time": "2024-11-26T03:11:00.944173Z"
    }
   },
   "source": [
    "graph_signal_matrix_filename = './data/38/38_quarter_single_16_18_20_dataset_astcgn.npz'\n",
    "batch_size = 32\n",
    "num_of_hours = 1\n",
    "\n",
    "train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, stats_data = load_graphdata_channel1(\n",
    "    graph_signal_matrix_filename, num_of_hours, batch_size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load file: ./data/38/38_quarter_single_16_18_20_dataset_astcgn.npz\n",
      "train: torch.Size([297, 38, 2, 5]) torch.Size([297, 38, 1])\n",
      "val: torch.Size([99, 38, 2, 5]) torch.Size([99, 38, 1])\n",
      "test: torch.Size([99, 38, 2, 5]) torch.Size([99, 38, 1])\n",
      "stats_data: {'signal_0_mean': array([[[[29.81448662],\n",
      "         [16.        ]]]]), 'signal_0_std': array([[[[7.38926896],\n",
      "         [0.        ]]]]), 'signal_1_mean': array([[[[30.01646199],\n",
      "         [18.        ]]]]), 'signal_1_std': array([[[[7.41955276],\n",
      "         [0.        ]]]]), 'signal_2_mean': array([[[[30.21835141],\n",
      "         [20.        ]]]]), 'signal_2_std': array([[[[7.45859301],\n",
      "         [0.        ]]]])}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "00976f32-67b4-455e-8679-b94b1ebaba82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:39.008565Z",
     "start_time": "2024-11-07T03:15:39.004681Z"
    }
   },
   "source": [
    "def get_adjacency_matrix(distance_df_filename, num_of_vertices, id_filename=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_df_filename: str, path of the csv file contains edges information\n",
    "    num_of_vertices: int, the number of vertices\n",
    "    Returns\n",
    "    ----------\n",
    "    A: np.ndarray, adjacency matrix\n",
    "    '''\n",
    "    if 'npy' in distance_df_filename:  # false\n",
    "        adj_mx = np.load(distance_df_filename)\n",
    "        return adj_mx, None\n",
    "    else:\n",
    "\n",
    "        #--------------------------------------------- read from here\n",
    "        import csv\n",
    "        A = np.zeros((int(num_of_vertices), int(num_of_vertices)), dtype=np.float32)\n",
    "        distaneA = np.zeros((int(num_of_vertices), int(num_of_vertices)), dtype=np.float32)\n",
    "\n",
    "        #------------ Ignore\n",
    "        if id_filename:  # false\n",
    "            with open(id_filename, 'r') as f:\n",
    "                id_dict = {int(i): idx for idx, i in enumerate(f.read().strip().split('\\n'))}  # 把节点id（idx）映射成从0开始的索引\n",
    "\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[id_dict[i], id_dict[j]] = 1\n",
    "                    distaneA[id_dict[i], id_dict[j]] = distance\n",
    "            return A, distaneA\n",
    "\n",
    "        else:\n",
    "            #-------------Continue reading\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[i, j] = 1\n",
    "                    distaneA[i, j] = distance\n",
    "            return A, distaneA"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "854a2a4b-e6d8-443b-b4f1-9b2a0ffa3d6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:39.680471Z",
     "start_time": "2024-11-07T03:15:39.677336Z"
    }
   },
   "source": [
    "id_filename = './data/62/62_id_filename.csv'\n",
    "# 邻接矩阵文件的路径，即存储交通网络连接信息的文件。\n",
    "adj_filename = './data/62/62_node_distance.csv'\n",
    "# num_of_vertices：节点的数量，即传感器的数量\n",
    "num_of_vertices = 62  #62为混凝土内温度节点数量  总数量为66含进出水温节点\n",
    "# get_adjacency_matrix：用于读取邻接矩阵文件并生成邻接矩阵adj_mx和距离矩阵distance_mx。邻接矩阵adj_mx是一个66x66的矩阵，表示节点之间的连接关系。\n",
    "adj_mx, distance_mx = get_adjacency_matrix(adj_filename, num_of_vertices, id_filename)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用 networkx 库创建图并绘制出来。",
   "id": "c4897f9a8e852758"
  },
  {
   "cell_type": "code",
   "id": "ba3aea8f-ccab-4957-a5fe-0b21cf0f9dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:40.879394Z",
     "start_time": "2024-11-07T03:15:40.873415Z"
    }
   },
   "source": [
    "# 使用 networkx 库创建图并绘制出来。\n",
    "# 找到邻接矩阵中值为1的位置，这些位置表示节点之间的连接。\n",
    "# rows, cols = np.where(adj_mx == 1)\n",
    "# # 将行和列索引组合成边的列表。\n",
    "# edges = zip(rows.tolist(), cols.tolist())\n",
    "# gr = nx.Graph()\n",
    "# gr.add_edges_from(edges)\n",
    "# nx.draw(gr, node_size=3)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:41.461482Z",
     "start_time": "2024-11-07T03:15:41.457905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建PyTorch张量表示边的索引\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "edges = zip(rows.tolist(), cols.tolist())  # 将行和列索引组合成边的列表。\n",
    "# torch.LongTensor(np.array([rows, cols]))：将边的行和列索引转换为PyTorch的长整型张量。\n",
    "edge_index_data = torch.LongTensor(np.array([rows, cols])).to(DEVICE)"
   ],
   "id": "4bd123b1f809834c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:41.933198Z",
     "start_time": "2024-11-07T03:15:41.910345Z"
    }
   },
   "cell_type": "code",
   "source": "from torch_geometric_temporal.nn.attention import ASTGCN",
   "id": "a83e2eec367a5fef",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:42.404727Z",
     "start_time": "2024-11-07T03:15:42.378751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nb_block = 2\n",
    "in_channels = 1\n",
    "K = 3\n",
    "nb_chev_filter = 64\n",
    "nb_time_filter = 64\n",
    "# time_strides = num_of_hours\n",
    "time_strides = 1\n",
    "num_for_predict = 3\n",
    "len_input = 15\n",
    "#L_tilde = scaled_Laplacian(adj_mx)\n",
    "#cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(DEVICE) for i in cheb_polynomial(L_tilde, K)]\n",
    "\n",
    "net = ASTGCN(nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, num_for_predict, len_input,\n",
    "             num_of_vertices).to(DEVICE)\n",
    "'''\n",
    "params:\n",
    "    nb_block – Number of ASTGCN blocks in the model.\n",
    "    in_channels – Number of input features.\n",
    "    K – Order of Chebyshev polynomials. Degree is K-1.\n",
    "    time_strides – Time strides during temporal convolution.\n",
    "    num_for_predict – Number of predictions to make in the future.\n",
    "    len_input – Length of the input sequence.\n",
    "    num_of_vertices – Number of vertices in the graph.\n",
    "    normalization – The normalization scheme for the graph\n",
    "'''\n",
    "print(net)"
   ],
   "id": "c93041300b64c09e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTGCN(\n",
      "  (_blocklist): ModuleList(\n",
      "    (0): ASTGCNBlock(\n",
      "      (_temporal_attention): TemporalAttention()\n",
      "      (_spatial_attention): SpatialAttention()\n",
      "      (_chebconv_attention): ChebConvAttention(1, 64, K=3, normalization=None)\n",
      "      (_time_convolution): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
      "      (_residual_convolution): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): ASTGCNBlock(\n",
      "      (_temporal_attention): TemporalAttention()\n",
      "      (_spatial_attention): SpatialAttention()\n",
      "      (_chebconv_attention): ChebConvAttention(64, 64, K=3, normalization=None)\n",
      "      (_time_convolution): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
      "      (_residual_convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (_final_conv): Conv2d(15, 3, kernel_size=(1, 64), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:43.403078Z",
     "start_time": "2024-11-07T03:15:43.390336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印模型的所有参数及其名称\n",
    "for name, param in net.named_parameters():\n",
    "    print(f\"Layer: {name} | Shape: {param.shape} | Requires Grad: {param.requires_grad}\")"
   ],
   "id": "556a6ee2cd137967",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: _blocklist.0._temporal_attention._U1 | Shape: torch.Size([62]) | Requires Grad: True\n",
      "Layer: _blocklist.0._temporal_attention._U2 | Shape: torch.Size([1, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.0._temporal_attention._U3 | Shape: torch.Size([1]) | Requires Grad: True\n",
      "Layer: _blocklist.0._temporal_attention._be | Shape: torch.Size([1, 15, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.0._temporal_attention._Ve | Shape: torch.Size([15, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._W1 | Shape: torch.Size([15]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._W2 | Shape: torch.Size([1, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._W3 | Shape: torch.Size([1]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._bs | Shape: torch.Size([1, 62, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.0._spatial_attention._Vs | Shape: torch.Size([62, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.0._chebconv_attention._weight | Shape: torch.Size([3, 1, 64]) | Requires Grad: True\n",
      "Layer: _blocklist.0._chebconv_attention._bias | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.0._time_convolution.weight | Shape: torch.Size([64, 64, 1, 3]) | Requires Grad: True\n",
      "Layer: _blocklist.0._time_convolution.bias | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.0._residual_convolution.weight | Shape: torch.Size([64, 1, 1, 1]) | Requires Grad: True\n",
      "Layer: _blocklist.0._residual_convolution.bias | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.0._layer_norm.weight | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.0._layer_norm.bias | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._U1 | Shape: torch.Size([62]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._U2 | Shape: torch.Size([64, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._U3 | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._be | Shape: torch.Size([1, 15, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.1._temporal_attention._Ve | Shape: torch.Size([15, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._W1 | Shape: torch.Size([15]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._W2 | Shape: torch.Size([64, 15]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._W3 | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._bs | Shape: torch.Size([1, 62, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.1._spatial_attention._Vs | Shape: torch.Size([62, 62]) | Requires Grad: True\n",
      "Layer: _blocklist.1._chebconv_attention._weight | Shape: torch.Size([3, 64, 64]) | Requires Grad: True\n",
      "Layer: _blocklist.1._chebconv_attention._bias | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.1._time_convolution.weight | Shape: torch.Size([64, 64, 1, 3]) | Requires Grad: True\n",
      "Layer: _blocklist.1._time_convolution.bias | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.1._residual_convolution.weight | Shape: torch.Size([64, 64, 1, 1]) | Requires Grad: True\n",
      "Layer: _blocklist.1._residual_convolution.bias | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.1._layer_norm.weight | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _blocklist.1._layer_norm.bias | Shape: torch.Size([64]) | Requires Grad: True\n",
      "Layer: _final_conv.weight | Shape: torch.Size([3, 15, 1, 64]) | Requires Grad: True\n",
      "Layer: _final_conv.bias | Shape: torch.Size([3]) | Requires Grad: True\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:44.019704Z",
     "start_time": "2024-11-07T03:15:44.000176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "print('Net\\'s state_dict:')\n",
    "total_param = 0\n",
    "'''\n",
    "state_dict() 方法返回一个字典，其中包含模型的所有参数和缓冲区\n",
    "param_tensor 是参数的名称。\n",
    "param_size 是参数张量的大小。\n",
    "param_device 是参数张量所在的设备。\n",
    "np.prod(param_size) 计算参数张量中所有元素的乘积，即参数的总数。\n",
    "'''\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, '\\t', net.state_dict()[param_tensor].size(), '\\t', net.state_dict()[param_tensor].device)\n",
    "    total_param += np.prod(net.state_dict()[param_tensor].size())\n",
    "print('Net\\'s total params:', total_param)\n",
    "#--------------------------------------------------\n",
    "print('Optimizer\\'s state_dict:')\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, '\\t', optimizer.state_dict()[var_name])"
   ],
   "id": "ff6e8dc7f85deee1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net's state_dict:\n",
      "_blocklist.0._temporal_attention._U1 \t torch.Size([62]) \t cuda:0\n",
      "_blocklist.0._temporal_attention._U2 \t torch.Size([1, 62]) \t cuda:0\n",
      "_blocklist.0._temporal_attention._U3 \t torch.Size([1]) \t cuda:0\n",
      "_blocklist.0._temporal_attention._be \t torch.Size([1, 15, 15]) \t cuda:0\n",
      "_blocklist.0._temporal_attention._Ve \t torch.Size([15, 15]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._W1 \t torch.Size([15]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._W2 \t torch.Size([1, 15]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._W3 \t torch.Size([1]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._bs \t torch.Size([1, 62, 62]) \t cuda:0\n",
      "_blocklist.0._spatial_attention._Vs \t torch.Size([62, 62]) \t cuda:0\n",
      "_blocklist.0._chebconv_attention._weight \t torch.Size([3, 1, 64]) \t cuda:0\n",
      "_blocklist.0._chebconv_attention._bias \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.0._time_convolution.weight \t torch.Size([64, 64, 1, 3]) \t cuda:0\n",
      "_blocklist.0._time_convolution.bias \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.0._residual_convolution.weight \t torch.Size([64, 1, 1, 1]) \t cuda:0\n",
      "_blocklist.0._residual_convolution.bias \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.0._layer_norm.weight \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.0._layer_norm.bias \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._U1 \t torch.Size([62]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._U2 \t torch.Size([64, 62]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._U3 \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._be \t torch.Size([1, 15, 15]) \t cuda:0\n",
      "_blocklist.1._temporal_attention._Ve \t torch.Size([15, 15]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._W1 \t torch.Size([15]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._W2 \t torch.Size([64, 15]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._W3 \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._bs \t torch.Size([1, 62, 62]) \t cuda:0\n",
      "_blocklist.1._spatial_attention._Vs \t torch.Size([62, 62]) \t cuda:0\n",
      "_blocklist.1._chebconv_attention._weight \t torch.Size([3, 64, 64]) \t cuda:0\n",
      "_blocklist.1._chebconv_attention._bias \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.1._time_convolution.weight \t torch.Size([64, 64, 1, 3]) \t cuda:0\n",
      "_blocklist.1._time_convolution.bias \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.1._residual_convolution.weight \t torch.Size([64, 64, 1, 1]) \t cuda:0\n",
      "_blocklist.1._residual_convolution.bias \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.1._layer_norm.weight \t torch.Size([64]) \t cuda:0\n",
      "_blocklist.1._layer_norm.bias \t torch.Size([64]) \t cuda:0\n",
      "_final_conv.weight \t torch.Size([3, 15, 1, 64]) \t cuda:0\n",
      "_final_conv.bias \t torch.Size([3]) \t cuda:0\n",
      "Net's total params: 66304\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]}]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:44.855042Z",
     "start_time": "2024-11-07T03:15:44.844602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def masked_mae(preds, labels, null_val=np.nan):\n",
    "    '''\n",
    "    :param: preds（预测值）\n",
    "            labels（真实标签）\n",
    "            null_val（表示无效值的标记，默认是 NaN）\n",
    "    '''\n",
    "    if np.isnan(null_val):\n",
    "        # 生成一个掩码 mask，掩码中的每个元素表示 labels 中对应位置是否不是 NaN（即 True 表示不是 NaN，False 表示是 NaN）\n",
    "        # ~ 按位取反运算符\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        # 生成一个掩码 mask，掩码中的每个元素表示 labels 中对应位置是否不等于 null_val\n",
    "        mask = (labels != null_val)\n",
    "    mask = mask.float()\n",
    "    mask /= torch.mean((mask))\n",
    "\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    # 计算绝对误差: 计算预测值 preds 和真实标签 labels 之间的绝对误差\n",
    "\n",
    "    # print('preds.shape:', preds.shape)\n",
    "    # print('labels.shape:', labels.shape)\n",
    "\n",
    "    loss = torch.abs(preds - labels)\n",
    "    # 应用掩码: 将绝对误差与掩码相乘，这样掩码为 0 的位置（即无效值位置）的误差将被忽略（置为 0）\n",
    "    loss = loss * mask\n",
    "    # 处理误差中的 NaN 值: 将误差中的 NaN 值替换为 0。这是为了防止后续计算中出现 NaN 值\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    # 返回带掩码的 MAE: 计算并返回误差的均值，即带掩码的平均绝对误差\n",
    "    return torch.mean(loss)"
   ],
   "id": "e88604fa28faf102",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RMSE（均方根误差）和 MAE（平均绝对误差）是两种常用的评估回归模型性能的指标。它们分别衡量预测值与实际值之间的差异，具有不同的特性和适用场景。\n",
    "---\n",
    "#### RMSE（Root Mean Squared Error）\n",
    "\n",
    "RMSE 是预测值与实际值之间的均方根误差，计算公式为：\n",
    "\n",
    "![image-20241107102637266](https://hexo-ufoozhenghao.oss-cn-beijing.aliyuncs.com/BlogHexo/image-20241107102637266.png)\n",
    "\n",
    "其中：\n",
    "\n",
    "- n*n* 是样本数量。\n",
    "- yi*y**i* 是第 i*i* 个样本的实际值。\n",
    "- y^i*y*^*i* 是第 i*i* 个样本的预测值。\n",
    "\n",
    "##### 特点\n",
    "\n",
    "- **敏感度**：对大的误差特别敏感，因为平方项会放大大的误差。\n",
    "- **单位**：与原始数据的单位相同。\n",
    "\n",
    "##### 适用场景\n",
    "\n",
    "- 当需要对大的误差进行特别关注时使用。\n",
    "- 更适合在误差呈正态分布时使用。\n",
    "\n",
    "#### MAE（Mean Absolute Error）\n",
    "\n",
    "MAE 是预测值与实际值之间的平均绝对误差，计算公式为：\n",
    "\n",
    "![image-20241107102501396](https://hexo-ufoozhenghao.oss-cn-beijing.aliyuncs.com/BlogHexo/image-20241107102501396.png)\n",
    "\n",
    "其中：\n",
    "\n",
    "- n*n* 是样本数量。\n",
    "- yi*y**i* 是第 i*i* 个样本的实际值。\n",
    "- y^i*y*^*i* 是第 i*i* 个样本的预测值。\n",
    "\n",
    "##### 特点\n",
    "\n",
    "- **敏感度**：对大的误差不如 RMSE 敏感，因为绝对值不会放大大的误差。\n",
    "- **单位**：与原始数据的单位相同。\n",
    "\n",
    "##### 适用场景\n",
    "\n",
    "- 当需要对所有误差进行均等关注时使用。\n",
    "- 更适合在误差分布较均匀时使用。\n",
    "\n",
    "#### 比较\n",
    "\n",
    "1. **对异常值的敏感度**：\n",
    "   - RMSE 对异常值更加敏感，因为平方项会放大大的误差。\n",
    "   - MAE 对异常值不如 RMSE 敏感，因为绝对值不会放大大的误差。\n",
    "2. **解释性**：\n",
    "   - RMSE 更容易解释，因为它与原始数据的单位相同，并且对大的误差更敏感。\n",
    "   - MAE 也容易解释，但它对所有误差的权重相同。\n",
    "3. **选择标准**：\n",
    "   - 如果希望对大的误差进行更多的惩罚，使用 RMSE。\n",
    "   - 如果希望对所有误差进行均等的权重，使用 MAE。\n",
    "\n",
    "### 使用掩码：适用于处理缺失数据或无效数据的场景，通过忽略无效数据点，提高模型的鲁棒性和灵活性。\n",
    "### 不使用掩码：适用于数据集完整且无缺失值的场景，损失函数直接应用于所有数据点。"
   ],
   "id": "9f5aee486f293a99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:56.717221Z",
     "start_time": "2024-11-07T03:15:56.711366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "masked_flag = 0\n",
    "criterion = nn.L1Loss().to(DEVICE)  # 初始化为 L1 损失函数（MAE，平均绝对误差）。\n",
    "criterion_masked = masked_mae\n",
    "loss_function = 'mse'\n",
    "\n",
    "metric_method = 'unmask'\n",
    "missing_value = 0.0\n",
    "\n",
    "if loss_function == 'masked_mse':\n",
    "    criterion_masked = masked_mse  #nn.MSELoss().to(DEVICE)\n",
    "    masked_flag = 1\n",
    "elif loss_function == 'masked_mae':\n",
    "    criterion_masked = masked_mae\n",
    "    masked_flag = 1\n",
    "elif loss_function == 'mae':\n",
    "    criterion = nn.L1Loss().to(DEVICE)  # MAE 损失函数\n",
    "    masked_flag = 0\n",
    "elif loss_function == 'rmse':\n",
    "    criterion = nn.MSELoss().to(DEVICE)  # MSE 损失函数\n",
    "    masked_flag = 0"
   ],
   "id": "6f187528d49ead9d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:57.697295Z",
     "start_time": "2024-11-07T03:15:57.688160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_val_loss_mstgcn(net, val_loader, criterion, masked_flag, missing_value, sw, epoch, edge_index_data,\n",
    "                            limit=None):\n",
    "    '''\n",
    "    for rnn, compute mean loss on validation set\n",
    "    :param net: model\n",
    "    :param val_loader: torch.utils.data.utils.DataLoader\n",
    "    :param criterion: torch.nn.MSELoss\n",
    "    :param sw: tensorboardX.SummaryWriter\n",
    "    :param global_step: int, current global_step\n",
    "    :param limit: int,\n",
    "    :return: val_loss\n",
    "    '''\n",
    "    # 将模型设置为评估模式 net.eval()\n",
    "    net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loader_length = len(val_loader)  # nb of batch\n",
    "        tmp = []  # batch loss\n",
    "        for batch_index, batch_data in enumerate(val_loader):\n",
    "            encoder_inputs, labels = batch_data\n",
    "            outputs = net(encoder_inputs, edge_index_data)\n",
    "            # 根据 masked_flag 来选择合适的损失函数。如果 masked_flag 为真，则使用带掩码的损失函数，否则使用标准损失函数。\n",
    "            if masked_flag:\n",
    "                loss = criterion(outputs, labels, missing_value)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            tmp.append(loss.item())\n",
    "            if batch_index % 100 == 0:\n",
    "                print('validation batch %s / %s, loss: %.2f' % (batch_index + 1, val_loader_length, loss.item()))\n",
    "            if (limit is not None) and batch_index >= limit:\n",
    "                break\n",
    "\n",
    "        validation_loss = sum(tmp) / len(tmp)\n",
    "        sw.add_scalar('validation_loss', validation_loss, epoch)\n",
    "    return validation_loss"
   ],
   "id": "86bbdfc1071212fd",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:15:59.449649Z",
     "start_time": "2024-11-07T03:15:59.442522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "global_step = 0\n",
    "best_epoch = 0\n",
    "# np.inf 表示正无穷大\n",
    "best_val_loss = np.inf\n",
    "start_time = time()"
   ],
   "id": "aafbe0016c8c3192",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "astgcn.py:\n",
    "/home/nevo/miniconda3/envs/pyg-ubuntu/lib/python3.10/site-packages/torch_geometric_temporal/nn/attention/astgcn.py"
   ],
   "id": "24843640ce613300"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T03:16:12.699403Z",
     "start_time": "2024-11-07T03:16:00.624974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train model\n",
    "for epoch in range(200):\n",
    "    params_filename = os.path.join('./data/62/train_params/', 'epoch_%s.params' % epoch)\n",
    "    masked_flag = 1\n",
    "    if masked_flag:\n",
    "        val_loss = compute_val_loss_mstgcn(net, val_loader, criterion_masked, masked_flag, missing_value, sw, epoch,\n",
    "                                           edge_index_data)\n",
    "    else:\n",
    "        val_loss = compute_val_loss_mstgcn(net, val_loader, criterion, masked_flag, missing_value, sw, epoch,\n",
    "                                           edge_index_data)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(net.state_dict(), params_filename)\n",
    "        print('save parameters to file: %s' % params_filename)\n",
    "\n",
    "    # 训练模式\n",
    "    net.train()  # ensure dropout layers are in train mode\n",
    "\n",
    "    for batch_index, batch_data in enumerate(train_loader):\n",
    "        encoder_inputs, labels = batch_data  # encoder_inputs torch.Size([32, 307, 1, 12])  label torch.Size([32, 307, 12])\n",
    "        # 清除上一步的梯度。\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播计算模型输出\n",
    "        outputs = net(encoder_inputs, edge_index_data)  # torch.Size([32, 307, 12])\n",
    "\n",
    "        if masked_flag:\n",
    "            loss = criterion_masked(outputs, labels, missing_value)\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播计算梯度\n",
    "        loss.backward()\n",
    "        #  更新模型参数\n",
    "        optimizer.step()\n",
    "        training_loss = loss.item()\n",
    "        global_step += 1\n",
    "        sw.add_scalar('training_loss', training_loss, global_step)\n",
    "\n",
    "        if global_step % 200 == 0:\n",
    "            print(\n",
    "                'global step: %s, training loss: %.2f, time: %.2fs' % (global_step, training_loss, time() - start_time))"
   ],
   "id": "65dba54e02357c50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation batch 1 / 1, loss: 22.86\n",
      "save parameters to file: ./data/62/train_params/epoch_0.params\n",
      "validation batch 1 / 1, loss: 22.77\n",
      "save parameters to file: ./data/62/train_params/epoch_1.params\n",
      "validation batch 1 / 1, loss: 22.69\n",
      "save parameters to file: ./data/62/train_params/epoch_2.params\n",
      "validation batch 1 / 1, loss: 22.60\n",
      "save parameters to file: ./data/62/train_params/epoch_3.params\n",
      "validation batch 1 / 1, loss: 22.51\n",
      "save parameters to file: ./data/62/train_params/epoch_4.params\n",
      "validation batch 1 / 1, loss: 22.43\n",
      "save parameters to file: ./data/62/train_params/epoch_5.params\n",
      "validation batch 1 / 1, loss: 22.34\n",
      "save parameters to file: ./data/62/train_params/epoch_6.params\n",
      "validation batch 1 / 1, loss: 22.26\n",
      "save parameters to file: ./data/62/train_params/epoch_7.params\n",
      "validation batch 1 / 1, loss: 22.17\n",
      "save parameters to file: ./data/62/train_params/epoch_8.params\n",
      "validation batch 1 / 1, loss: 22.09\n",
      "save parameters to file: ./data/62/train_params/epoch_9.params\n",
      "validation batch 1 / 1, loss: 22.01\n",
      "save parameters to file: ./data/62/train_params/epoch_10.params\n",
      "validation batch 1 / 1, loss: 21.93\n",
      "save parameters to file: ./data/62/train_params/epoch_11.params\n",
      "validation batch 1 / 1, loss: 21.85\n",
      "save parameters to file: ./data/62/train_params/epoch_12.params\n",
      "validation batch 1 / 1, loss: 21.77\n",
      "save parameters to file: ./data/62/train_params/epoch_13.params\n",
      "validation batch 1 / 1, loss: 21.69\n",
      "save parameters to file: ./data/62/train_params/epoch_14.params\n",
      "validation batch 1 / 1, loss: 21.61\n",
      "save parameters to file: ./data/62/train_params/epoch_15.params\n",
      "validation batch 1 / 1, loss: 21.53\n",
      "save parameters to file: ./data/62/train_params/epoch_16.params\n",
      "validation batch 1 / 1, loss: 21.45\n",
      "save parameters to file: ./data/62/train_params/epoch_17.params\n",
      "validation batch 1 / 1, loss: 21.37\n",
      "save parameters to file: ./data/62/train_params/epoch_18.params\n",
      "validation batch 1 / 1, loss: 21.29\n",
      "save parameters to file: ./data/62/train_params/epoch_19.params\n",
      "validation batch 1 / 1, loss: 21.21\n",
      "save parameters to file: ./data/62/train_params/epoch_20.params\n",
      "validation batch 1 / 1, loss: 21.13\n",
      "save parameters to file: ./data/62/train_params/epoch_21.params\n",
      "validation batch 1 / 1, loss: 21.06\n",
      "save parameters to file: ./data/62/train_params/epoch_22.params\n",
      "validation batch 1 / 1, loss: 20.98\n",
      "save parameters to file: ./data/62/train_params/epoch_23.params\n",
      "validation batch 1 / 1, loss: 20.90\n",
      "save parameters to file: ./data/62/train_params/epoch_24.params\n",
      "validation batch 1 / 1, loss: 20.82\n",
      "save parameters to file: ./data/62/train_params/epoch_25.params\n",
      "validation batch 1 / 1, loss: 20.75\n",
      "save parameters to file: ./data/62/train_params/epoch_26.params\n",
      "validation batch 1 / 1, loss: 20.67\n",
      "save parameters to file: ./data/62/train_params/epoch_27.params\n",
      "validation batch 1 / 1, loss: 20.59\n",
      "save parameters to file: ./data/62/train_params/epoch_28.params\n",
      "validation batch 1 / 1, loss: 20.52\n",
      "save parameters to file: ./data/62/train_params/epoch_29.params\n",
      "validation batch 1 / 1, loss: 20.44\n",
      "save parameters to file: ./data/62/train_params/epoch_30.params\n",
      "validation batch 1 / 1, loss: 20.36\n",
      "save parameters to file: ./data/62/train_params/epoch_31.params\n",
      "validation batch 1 / 1, loss: 20.28\n",
      "save parameters to file: ./data/62/train_params/epoch_32.params\n",
      "validation batch 1 / 1, loss: 20.21\n",
      "save parameters to file: ./data/62/train_params/epoch_33.params\n",
      "validation batch 1 / 1, loss: 20.13\n",
      "save parameters to file: ./data/62/train_params/epoch_34.params\n",
      "validation batch 1 / 1, loss: 20.05\n",
      "save parameters to file: ./data/62/train_params/epoch_35.params\n",
      "validation batch 1 / 1, loss: 19.97\n",
      "save parameters to file: ./data/62/train_params/epoch_36.params\n",
      "validation batch 1 / 1, loss: 19.89\n",
      "save parameters to file: ./data/62/train_params/epoch_37.params\n",
      "validation batch 1 / 1, loss: 19.81\n",
      "save parameters to file: ./data/62/train_params/epoch_38.params\n",
      "validation batch 1 / 1, loss: 19.73\n",
      "save parameters to file: ./data/62/train_params/epoch_39.params\n",
      "validation batch 1 / 1, loss: 19.65\n",
      "save parameters to file: ./data/62/train_params/epoch_40.params\n",
      "validation batch 1 / 1, loss: 19.57\n",
      "save parameters to file: ./data/62/train_params/epoch_41.params\n",
      "validation batch 1 / 1, loss: 19.49\n",
      "save parameters to file: ./data/62/train_params/epoch_42.params\n",
      "validation batch 1 / 1, loss: 19.41\n",
      "save parameters to file: ./data/62/train_params/epoch_43.params\n",
      "validation batch 1 / 1, loss: 19.33\n",
      "save parameters to file: ./data/62/train_params/epoch_44.params\n",
      "validation batch 1 / 1, loss: 19.25\n",
      "save parameters to file: ./data/62/train_params/epoch_45.params\n",
      "validation batch 1 / 1, loss: 19.17\n",
      "save parameters to file: ./data/62/train_params/epoch_46.params\n",
      "validation batch 1 / 1, loss: 19.09\n",
      "save parameters to file: ./data/62/train_params/epoch_47.params\n",
      "validation batch 1 / 1, loss: 19.00\n",
      "save parameters to file: ./data/62/train_params/epoch_48.params\n",
      "validation batch 1 / 1, loss: 18.92\n",
      "save parameters to file: ./data/62/train_params/epoch_49.params\n",
      "validation batch 1 / 1, loss: 18.84\n",
      "save parameters to file: ./data/62/train_params/epoch_50.params\n",
      "validation batch 1 / 1, loss: 18.75\n",
      "save parameters to file: ./data/62/train_params/epoch_51.params\n",
      "validation batch 1 / 1, loss: 18.67\n",
      "save parameters to file: ./data/62/train_params/epoch_52.params\n",
      "validation batch 1 / 1, loss: 18.58\n",
      "save parameters to file: ./data/62/train_params/epoch_53.params\n",
      "validation batch 1 / 1, loss: 18.50\n",
      "save parameters to file: ./data/62/train_params/epoch_54.params\n",
      "validation batch 1 / 1, loss: 18.41\n",
      "save parameters to file: ./data/62/train_params/epoch_55.params\n",
      "validation batch 1 / 1, loss: 18.33\n",
      "save parameters to file: ./data/62/train_params/epoch_56.params\n",
      "validation batch 1 / 1, loss: 18.24\n",
      "save parameters to file: ./data/62/train_params/epoch_57.params\n",
      "validation batch 1 / 1, loss: 18.15\n",
      "save parameters to file: ./data/62/train_params/epoch_58.params\n",
      "validation batch 1 / 1, loss: 18.06\n",
      "save parameters to file: ./data/62/train_params/epoch_59.params\n",
      "validation batch 1 / 1, loss: 17.97\n",
      "save parameters to file: ./data/62/train_params/epoch_60.params\n",
      "validation batch 1 / 1, loss: 17.88\n",
      "save parameters to file: ./data/62/train_params/epoch_61.params\n",
      "validation batch 1 / 1, loss: 17.79\n",
      "save parameters to file: ./data/62/train_params/epoch_62.params\n",
      "validation batch 1 / 1, loss: 17.70\n",
      "save parameters to file: ./data/62/train_params/epoch_63.params\n",
      "validation batch 1 / 1, loss: 17.60\n",
      "save parameters to file: ./data/62/train_params/epoch_64.params\n",
      "validation batch 1 / 1, loss: 17.51\n",
      "save parameters to file: ./data/62/train_params/epoch_65.params\n",
      "validation batch 1 / 1, loss: 17.41\n",
      "save parameters to file: ./data/62/train_params/epoch_66.params\n",
      "validation batch 1 / 1, loss: 17.32\n",
      "save parameters to file: ./data/62/train_params/epoch_67.params\n",
      "validation batch 1 / 1, loss: 17.22\n",
      "save parameters to file: ./data/62/train_params/epoch_68.params\n",
      "validation batch 1 / 1, loss: 17.12\n",
      "save parameters to file: ./data/62/train_params/epoch_69.params\n",
      "validation batch 1 / 1, loss: 17.02\n",
      "save parameters to file: ./data/62/train_params/epoch_70.params\n",
      "validation batch 1 / 1, loss: 16.92\n",
      "save parameters to file: ./data/62/train_params/epoch_71.params\n",
      "validation batch 1 / 1, loss: 16.82\n",
      "save parameters to file: ./data/62/train_params/epoch_72.params\n",
      "validation batch 1 / 1, loss: 16.72\n",
      "save parameters to file: ./data/62/train_params/epoch_73.params\n",
      "validation batch 1 / 1, loss: 16.62\n",
      "save parameters to file: ./data/62/train_params/epoch_74.params\n",
      "validation batch 1 / 1, loss: 16.52\n",
      "save parameters to file: ./data/62/train_params/epoch_75.params\n",
      "validation batch 1 / 1, loss: 16.41\n",
      "save parameters to file: ./data/62/train_params/epoch_76.params\n",
      "validation batch 1 / 1, loss: 16.31\n",
      "save parameters to file: ./data/62/train_params/epoch_77.params\n",
      "validation batch 1 / 1, loss: 16.21\n",
      "save parameters to file: ./data/62/train_params/epoch_78.params\n",
      "validation batch 1 / 1, loss: 16.10\n",
      "save parameters to file: ./data/62/train_params/epoch_79.params\n",
      "validation batch 1 / 1, loss: 16.00\n",
      "save parameters to file: ./data/62/train_params/epoch_80.params\n",
      "validation batch 1 / 1, loss: 15.89\n",
      "save parameters to file: ./data/62/train_params/epoch_81.params\n",
      "validation batch 1 / 1, loss: 15.79\n",
      "save parameters to file: ./data/62/train_params/epoch_82.params\n",
      "validation batch 1 / 1, loss: 15.68\n",
      "save parameters to file: ./data/62/train_params/epoch_83.params\n",
      "validation batch 1 / 1, loss: 15.57\n",
      "save parameters to file: ./data/62/train_params/epoch_84.params\n",
      "validation batch 1 / 1, loss: 15.47\n",
      "save parameters to file: ./data/62/train_params/epoch_85.params\n",
      "validation batch 1 / 1, loss: 15.36\n",
      "save parameters to file: ./data/62/train_params/epoch_86.params\n",
      "validation batch 1 / 1, loss: 15.25\n",
      "save parameters to file: ./data/62/train_params/epoch_87.params\n",
      "validation batch 1 / 1, loss: 15.14\n",
      "save parameters to file: ./data/62/train_params/epoch_88.params\n",
      "validation batch 1 / 1, loss: 15.04\n",
      "save parameters to file: ./data/62/train_params/epoch_89.params\n",
      "validation batch 1 / 1, loss: 14.93\n",
      "save parameters to file: ./data/62/train_params/epoch_90.params\n",
      "validation batch 1 / 1, loss: 14.82\n",
      "save parameters to file: ./data/62/train_params/epoch_91.params\n",
      "validation batch 1 / 1, loss: 14.71\n",
      "save parameters to file: ./data/62/train_params/epoch_92.params\n",
      "validation batch 1 / 1, loss: 14.60\n",
      "save parameters to file: ./data/62/train_params/epoch_93.params\n",
      "validation batch 1 / 1, loss: 14.49\n",
      "save parameters to file: ./data/62/train_params/epoch_94.params\n",
      "validation batch 1 / 1, loss: 14.38\n",
      "save parameters to file: ./data/62/train_params/epoch_95.params\n",
      "validation batch 1 / 1, loss: 14.27\n",
      "save parameters to file: ./data/62/train_params/epoch_96.params\n",
      "validation batch 1 / 1, loss: 14.16\n",
      "save parameters to file: ./data/62/train_params/epoch_97.params\n",
      "validation batch 1 / 1, loss: 14.05\n",
      "save parameters to file: ./data/62/train_params/epoch_98.params\n",
      "validation batch 1 / 1, loss: 13.94\n",
      "save parameters to file: ./data/62/train_params/epoch_99.params\n",
      "validation batch 1 / 1, loss: 13.83\n",
      "save parameters to file: ./data/62/train_params/epoch_100.params\n",
      "validation batch 1 / 1, loss: 13.72\n",
      "save parameters to file: ./data/62/train_params/epoch_101.params\n",
      "validation batch 1 / 1, loss: 13.61\n",
      "save parameters to file: ./data/62/train_params/epoch_102.params\n",
      "validation batch 1 / 1, loss: 13.50\n",
      "save parameters to file: ./data/62/train_params/epoch_103.params\n",
      "validation batch 1 / 1, loss: 13.39\n",
      "save parameters to file: ./data/62/train_params/epoch_104.params\n",
      "validation batch 1 / 1, loss: 13.28\n",
      "save parameters to file: ./data/62/train_params/epoch_105.params\n",
      "validation batch 1 / 1, loss: 13.17\n",
      "save parameters to file: ./data/62/train_params/epoch_106.params\n",
      "validation batch 1 / 1, loss: 13.06\n",
      "save parameters to file: ./data/62/train_params/epoch_107.params\n",
      "validation batch 1 / 1, loss: 12.95\n",
      "save parameters to file: ./data/62/train_params/epoch_108.params\n",
      "validation batch 1 / 1, loss: 12.84\n",
      "save parameters to file: ./data/62/train_params/epoch_109.params\n",
      "validation batch 1 / 1, loss: 12.73\n",
      "save parameters to file: ./data/62/train_params/epoch_110.params\n",
      "validation batch 1 / 1, loss: 12.62\n",
      "save parameters to file: ./data/62/train_params/epoch_111.params\n",
      "validation batch 1 / 1, loss: 12.51\n",
      "save parameters to file: ./data/62/train_params/epoch_112.params\n",
      "validation batch 1 / 1, loss: 12.39\n",
      "save parameters to file: ./data/62/train_params/epoch_113.params\n",
      "validation batch 1 / 1, loss: 12.28\n",
      "save parameters to file: ./data/62/train_params/epoch_114.params\n",
      "validation batch 1 / 1, loss: 12.17\n",
      "save parameters to file: ./data/62/train_params/epoch_115.params\n",
      "validation batch 1 / 1, loss: 12.06\n",
      "save parameters to file: ./data/62/train_params/epoch_116.params\n",
      "validation batch 1 / 1, loss: 11.94\n",
      "save parameters to file: ./data/62/train_params/epoch_117.params\n",
      "validation batch 1 / 1, loss: 11.83\n",
      "save parameters to file: ./data/62/train_params/epoch_118.params\n",
      "validation batch 1 / 1, loss: 11.72\n",
      "save parameters to file: ./data/62/train_params/epoch_119.params\n",
      "validation batch 1 / 1, loss: 11.60\n",
      "save parameters to file: ./data/62/train_params/epoch_120.params\n",
      "validation batch 1 / 1, loss: 11.49\n",
      "save parameters to file: ./data/62/train_params/epoch_121.params\n",
      "validation batch 1 / 1, loss: 11.37\n",
      "save parameters to file: ./data/62/train_params/epoch_122.params\n",
      "validation batch 1 / 1, loss: 11.26\n",
      "save parameters to file: ./data/62/train_params/epoch_123.params\n",
      "validation batch 1 / 1, loss: 11.14\n",
      "save parameters to file: ./data/62/train_params/epoch_124.params\n",
      "validation batch 1 / 1, loss: 11.03\n",
      "save parameters to file: ./data/62/train_params/epoch_125.params\n",
      "validation batch 1 / 1, loss: 10.91\n",
      "save parameters to file: ./data/62/train_params/epoch_126.params\n",
      "validation batch 1 / 1, loss: 10.80\n",
      "save parameters to file: ./data/62/train_params/epoch_127.params\n",
      "validation batch 1 / 1, loss: 10.68\n",
      "save parameters to file: ./data/62/train_params/epoch_128.params\n",
      "validation batch 1 / 1, loss: 10.56\n",
      "save parameters to file: ./data/62/train_params/epoch_129.params\n",
      "validation batch 1 / 1, loss: 10.45\n",
      "save parameters to file: ./data/62/train_params/epoch_130.params\n",
      "validation batch 1 / 1, loss: 10.33\n",
      "save parameters to file: ./data/62/train_params/epoch_131.params\n",
      "validation batch 1 / 1, loss: 10.22\n",
      "save parameters to file: ./data/62/train_params/epoch_132.params\n",
      "validation batch 1 / 1, loss: 10.10\n",
      "save parameters to file: ./data/62/train_params/epoch_133.params\n",
      "validation batch 1 / 1, loss: 9.99\n",
      "save parameters to file: ./data/62/train_params/epoch_134.params\n",
      "validation batch 1 / 1, loss: 9.87\n",
      "save parameters to file: ./data/62/train_params/epoch_135.params\n",
      "validation batch 1 / 1, loss: 9.75\n",
      "save parameters to file: ./data/62/train_params/epoch_136.params\n",
      "validation batch 1 / 1, loss: 9.64\n",
      "save parameters to file: ./data/62/train_params/epoch_137.params\n",
      "validation batch 1 / 1, loss: 9.52\n",
      "save parameters to file: ./data/62/train_params/epoch_138.params\n",
      "validation batch 1 / 1, loss: 9.41\n",
      "save parameters to file: ./data/62/train_params/epoch_139.params\n",
      "validation batch 1 / 1, loss: 9.30\n",
      "save parameters to file: ./data/62/train_params/epoch_140.params\n",
      "validation batch 1 / 1, loss: 9.18\n",
      "save parameters to file: ./data/62/train_params/epoch_141.params\n",
      "validation batch 1 / 1, loss: 9.07\n",
      "save parameters to file: ./data/62/train_params/epoch_142.params\n",
      "validation batch 1 / 1, loss: 8.95\n",
      "save parameters to file: ./data/62/train_params/epoch_143.params\n",
      "validation batch 1 / 1, loss: 8.84\n",
      "save parameters to file: ./data/62/train_params/epoch_144.params\n",
      "validation batch 1 / 1, loss: 8.73\n",
      "save parameters to file: ./data/62/train_params/epoch_145.params\n",
      "validation batch 1 / 1, loss: 8.62\n",
      "save parameters to file: ./data/62/train_params/epoch_146.params\n",
      "validation batch 1 / 1, loss: 8.50\n",
      "save parameters to file: ./data/62/train_params/epoch_147.params\n",
      "validation batch 1 / 1, loss: 8.39\n",
      "save parameters to file: ./data/62/train_params/epoch_148.params\n",
      "validation batch 1 / 1, loss: 8.28\n",
      "save parameters to file: ./data/62/train_params/epoch_149.params\n",
      "validation batch 1 / 1, loss: 8.17\n",
      "save parameters to file: ./data/62/train_params/epoch_150.params\n",
      "validation batch 1 / 1, loss: 8.06\n",
      "save parameters to file: ./data/62/train_params/epoch_151.params\n",
      "validation batch 1 / 1, loss: 7.95\n",
      "save parameters to file: ./data/62/train_params/epoch_152.params\n",
      "validation batch 1 / 1, loss: 7.84\n",
      "save parameters to file: ./data/62/train_params/epoch_153.params\n",
      "validation batch 1 / 1, loss: 7.73\n",
      "save parameters to file: ./data/62/train_params/epoch_154.params\n",
      "validation batch 1 / 1, loss: 7.61\n",
      "save parameters to file: ./data/62/train_params/epoch_155.params\n",
      "validation batch 1 / 1, loss: 7.51\n",
      "save parameters to file: ./data/62/train_params/epoch_156.params\n",
      "validation batch 1 / 1, loss: 7.40\n",
      "save parameters to file: ./data/62/train_params/epoch_157.params\n",
      "validation batch 1 / 1, loss: 7.29\n",
      "save parameters to file: ./data/62/train_params/epoch_158.params\n",
      "validation batch 1 / 1, loss: 7.19\n",
      "save parameters to file: ./data/62/train_params/epoch_159.params\n",
      "validation batch 1 / 1, loss: 7.08\n",
      "save parameters to file: ./data/62/train_params/epoch_160.params\n",
      "validation batch 1 / 1, loss: 6.98\n",
      "save parameters to file: ./data/62/train_params/epoch_161.params\n",
      "validation batch 1 / 1, loss: 6.88\n",
      "save parameters to file: ./data/62/train_params/epoch_162.params\n",
      "validation batch 1 / 1, loss: 6.78\n",
      "save parameters to file: ./data/62/train_params/epoch_163.params\n",
      "validation batch 1 / 1, loss: 6.69\n",
      "save parameters to file: ./data/62/train_params/epoch_164.params\n",
      "validation batch 1 / 1, loss: 6.60\n",
      "save parameters to file: ./data/62/train_params/epoch_165.params\n",
      "validation batch 1 / 1, loss: 6.52\n",
      "save parameters to file: ./data/62/train_params/epoch_166.params\n",
      "validation batch 1 / 1, loss: 6.45\n",
      "save parameters to file: ./data/62/train_params/epoch_167.params\n",
      "validation batch 1 / 1, loss: 6.37\n",
      "save parameters to file: ./data/62/train_params/epoch_168.params\n",
      "validation batch 1 / 1, loss: 6.30\n",
      "save parameters to file: ./data/62/train_params/epoch_169.params\n",
      "validation batch 1 / 1, loss: 6.23\n",
      "save parameters to file: ./data/62/train_params/epoch_170.params\n",
      "validation batch 1 / 1, loss: 6.16\n",
      "save parameters to file: ./data/62/train_params/epoch_171.params\n",
      "validation batch 1 / 1, loss: 6.10\n",
      "save parameters to file: ./data/62/train_params/epoch_172.params\n",
      "validation batch 1 / 1, loss: 6.04\n",
      "save parameters to file: ./data/62/train_params/epoch_173.params\n",
      "validation batch 1 / 1, loss: 5.98\n",
      "save parameters to file: ./data/62/train_params/epoch_174.params\n",
      "validation batch 1 / 1, loss: 5.94\n",
      "save parameters to file: ./data/62/train_params/epoch_175.params\n",
      "validation batch 1 / 1, loss: 5.89\n",
      "save parameters to file: ./data/62/train_params/epoch_176.params\n",
      "validation batch 1 / 1, loss: 5.85\n",
      "save parameters to file: ./data/62/train_params/epoch_177.params\n",
      "validation batch 1 / 1, loss: 5.81\n",
      "save parameters to file: ./data/62/train_params/epoch_178.params\n",
      "validation batch 1 / 1, loss: 5.78\n",
      "save parameters to file: ./data/62/train_params/epoch_179.params\n",
      "validation batch 1 / 1, loss: 5.74\n",
      "save parameters to file: ./data/62/train_params/epoch_180.params\n",
      "validation batch 1 / 1, loss: 5.71\n",
      "save parameters to file: ./data/62/train_params/epoch_181.params\n",
      "validation batch 1 / 1, loss: 5.68\n",
      "save parameters to file: ./data/62/train_params/epoch_182.params\n",
      "validation batch 1 / 1, loss: 5.66\n",
      "save parameters to file: ./data/62/train_params/epoch_183.params\n",
      "validation batch 1 / 1, loss: 5.65\n",
      "save parameters to file: ./data/62/train_params/epoch_184.params\n",
      "validation batch 1 / 1, loss: 5.63\n",
      "save parameters to file: ./data/62/train_params/epoch_185.params\n",
      "validation batch 1 / 1, loss: 5.62\n",
      "save parameters to file: ./data/62/train_params/epoch_186.params\n",
      "validation batch 1 / 1, loss: 5.61\n",
      "save parameters to file: ./data/62/train_params/epoch_187.params\n",
      "validation batch 1 / 1, loss: 5.60\n",
      "save parameters to file: ./data/62/train_params/epoch_188.params\n",
      "validation batch 1 / 1, loss: 5.59\n",
      "save parameters to file: ./data/62/train_params/epoch_189.params\n",
      "validation batch 1 / 1, loss: 5.57\n",
      "save parameters to file: ./data/62/train_params/epoch_190.params\n",
      "validation batch 1 / 1, loss: 5.56\n",
      "save parameters to file: ./data/62/train_params/epoch_191.params\n",
      "validation batch 1 / 1, loss: 5.55\n",
      "save parameters to file: ./data/62/train_params/epoch_192.params\n",
      "validation batch 1 / 1, loss: 5.54\n",
      "save parameters to file: ./data/62/train_params/epoch_193.params\n",
      "validation batch 1 / 1, loss: 5.53\n",
      "save parameters to file: ./data/62/train_params/epoch_194.params\n",
      "validation batch 1 / 1, loss: 5.51\n",
      "save parameters to file: ./data/62/train_params/epoch_195.params\n",
      "validation batch 1 / 1, loss: 5.50\n",
      "save parameters to file: ./data/62/train_params/epoch_196.params\n",
      "validation batch 1 / 1, loss: 5.49\n",
      "save parameters to file: ./data/62/train_params/epoch_197.params\n",
      "validation batch 1 / 1, loss: 5.48\n",
      "save parameters to file: ./data/62/train_params/epoch_198.params\n",
      "validation batch 1 / 1, loss: 5.47\n",
      "save parameters to file: ./data/62/train_params/epoch_199.params\n",
      "global step: 200, training loss: 10.79, time: 13.25s\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T01:22:59.849229Z",
     "start_time": "2024-11-07T01:22:59.816769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_loader_length = len(test_loader)  # nb of batch\n",
    "    tmp = []  # batch loss\n",
    "    for batch_index, batch_data in enumerate(test_loader):\n",
    "        encoder_inputs, labels = batch_data\n",
    "        outputs = net(encoder_inputs, edge_index_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        tmp.append(loss.item())\n",
    "        if batch_index % 100 == 0:\n",
    "            print('test_loss batch %s / %s, loss: %.2f' % (batch_index + 1, test_loader_length, loss.item()))\n",
    "\n",
    "    test_loss = sum(tmp) / len(tmp)\n",
    "    sw.add_scalar('test_loss', test_loss, epoch)\n",
    "print(test_loss)"
   ],
   "id": "b1912e37c5103150",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss batch 1 / 1, loss: 0.80\n",
      "0.8035534620285034\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pickig a random time point and visualizing the predictions of the first 50 detectors",
   "id": "a8131a468e81f7f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:41:12.869784Z",
     "start_time": "2024-11-06T14:41:12.862005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_output = outputs[0]  # prediction\n",
    "sample_labels = labels[0]  # truth\n",
    "print(sample_output.shape, sample_labels.shape)\n",
    "sample_labels[0][1]"
   ],
   "id": "7dff9c5f00cd83fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 3]) torch.Size([62, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(16.6600, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:41:13.552365Z",
     "start_time": "2024-11-06T14:41:13.449869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(30, 4), dpi=80)\n",
    "for i in range(50):\n",
    "    new_i = i * 3  # 调整步长为 3，以匹配 y 数据的长度\n",
    "    plt.plot(range(0 + new_i, 3 + new_i), sample_output[i].detach().cpu().numpy(), color='red')\n",
    "    plt.plot(range(0 + new_i, 3 + new_i), sample_labels[i].cpu().numpy(), color='blue')\n",
    "plt.show()"
   ],
   "id": "ef3906e90f37da88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x320 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9f2f38c44d032dba",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyg-ubuntu)",
   "language": "python",
   "name": "pyg-ubuntu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
